{"ID":"20251009102118-ov7p86j","Spec":"1","Type":"NodeDocument","Properties":{"custom-picgo-file-map-key":"\u0026#123;\u0026quot;3836c346af68ed4172472e5dbaf2a6ac\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251012165721-fsackoo.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;3836c346af68ed4172472e5dbaf2a6ac\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251012165721-fsackoo.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012165748.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;20251012162135-qza7x2g\u0026quot;\u0026#125;,\u0026quot;8ec3c219d386d40c793fd3e90fc5acd4\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251012170525-opr7jp8.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;8ec3c219d386d40c793fd3e90fc5acd4\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251012170525-opr7jp8.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012170526.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;\u0026quot;\u0026#125;\u0026#125;","icon":"1f427","id":"20251009102118-ov7p86j","title":"八股","type":"doc","updated":"20251019172748"},"Children":[{"ID":"20251009102119-bk79nfo","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20251009102119-bk79nfo","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"大模型基础"}]},{"ID":"20251009153935-3my4s8g","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20251009153935-3my4s8g","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"机制部分"}]},{"ID":"20251009153946-a1q9qdf","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251009153946-a1q9qdf","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"注意力"}]},{"ID":"20251012184734-qybbq7j","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20251012184734-qybbq7j","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"基础注意力"}]},{"ID":"20251009153412-yykcka0","Type":"NodeMathBlock","Properties":{"id":"20251009153412-yykcka0","updated":"20251009153923"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20251012230606-6yabwpj","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012230606-6yabwpj","updated":"20251012230607"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251009153923-8je37xh","Type":"NodeParagraph","Properties":{"id":"20251009153923-8je37xh","updated":"20251012230607"},"Children":[{"Type":"NodeText","Data":"注意力的过程?"}]},{"ID":"20251009154203-tuvpfcq","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251009154203-tuvpfcq","updated":"20251012230607"},"Children":[{"ID":"20251009154205-tkdo68p","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251009154205-tkdo68p","updated":"20251009154556"},"Children":[{"ID":"20251009154205-fglxmb0","Type":"NodeParagraph","Properties":{"id":"20251009154205-fglxmb0","updated":"20251009154556"},"Children":[{"Type":"NodeText","Data":"对于输入序列的每个位置，通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"点积"},{"Type":"NodeText","Data":"计算其与其他位置之间的相似度得分"}]}]},{"ID":"20251009154236-1a0l4zk","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251009154236-1a0l4zk","updated":"20251012230528"},"Children":[{"ID":"20251009154236-cvyxp3j","Type":"NodeParagraph","Properties":{"id":"20251009154236-cvyxp3j","updated":"20251012230528"},"Children":[{"Type":"NodeText","Data":"然后对得分进行放缩处理，以防止"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"梯度消失"}]}]},{"ID":"20251009171345-m7fwpby","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251009171345-m7fwpby","updated":"20251009171426"},"Children":[{"ID":"20251009171345-80siu0h","Type":"NodeParagraph","Properties":{"id":"20251009171345-80siu0h","updated":"20251009171426"},"Children":[{"Type":"NodeText","Data":"将得分用"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"softmax"},{"Type":"NodeText","Data":"​函数转换为注意力权重，以便计算每个位置的加权和"}]}]},{"ID":"20251012230421-yzfdo8i","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20251012230421-yzfdo8i","updated":"20251012230425"},"Children":[{"ID":"20251012230421-zk5iaf1","Type":"NodeParagraph","Properties":{"id":"20251012230421-zk5iaf1","updated":"20251012230425"},"Children":[{"Type":"NodeText","Data":"使用注意力权重对输入序列中的所有位置进行加权求和，得到每个位置的自注意输出。"}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251009154247-r0cts7t","Type":"NodeParagraph","Properties":{"id":"20251009154247-r0cts7t","updated":"20251009154247"}},{"ID":"20251012230551-0o9f2vy","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012230551-0o9f2vy","updated":"20251012231145"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251009154247-77nyjqd","Type":"NodeParagraph","Properties":{"id":"20251009154247-77nyjqd","updated":"20251012230552"},"Children":[{"Type":"NodeText","Data":"为什么要除以根号dk？能换成别的数吗？"}]},{"ID":"20251012162135-qza7x2g","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012162135-qza7x2g","updated":"20251012231145"},"Children":[{"ID":"20251012165751-2nkunul","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-2nkunul","updated":"20251012231145"},"Children":[{"ID":"20251012165751-zsw88nk","Type":"NodeParagraph","Properties":{"id":"20251012165751-zsw88nk","updated":"20251012231112"},"Children":[{"Type":"NodeText","Data":"避免softmax"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"梯度消失（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-card-success-background); color: var(--b3-card-success-color);"},"TextMarkType":"strong text mark","TextMarkTextContent":"注意不是爆炸"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-card-success-background); color: var(--b3-card-success-color);\"}"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"）"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"："}]},{"ID":"20251012165751-wnnnzy2","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012165751-wnnnzy2","updated":"20251012231145"},"Children":[{"ID":"20251012165751-ub9re1o","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-ub9re1o","updated":"20251012231137"},"Children":[{"ID":"20251012165751-cxkqkeg","Type":"NodeParagraph","Properties":{"id":"20251012165751-cxkqkeg","updated":"20251012231137"},"Children":[{"Type":"NodeText","Data":"当dk（Query/Key的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"维度"},{"Type":"NodeText","Data":"）"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"较大"},{"Type":"NodeText","Data":"时，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Q和K的点积结果会随"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"dk增大而显著变大"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);"},"TextMarkType":"em","TextMarkTextContent":"根号dk越大，维度变多，累加变多，导致qk点积的方差越大，导致向量元素值之间的差距变大"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);\"}"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"）。"}]}]},{"ID":"20251012165751-n3a6o1w","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012165751-n3a6o1w","updated":"20251012165751"},"Children":[{"ID":"20251012165751-9koqiuw","Type":"NodeParagraph","Properties":{"id":"20251012165751-9koqiuw","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"向量元素值之间的差距变大，代入softmax后进入softmax的饱和区域，容易出现某个值接近1，其他值接近0，从而导致梯度消失。"}]}]},{"ID":"20251012165751-nsny473","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251012165751-nsny473","updated":"20251012231145"},"Children":[{"ID":"20251012165751-7k38r6a","Type":"NodeParagraph","Properties":{"id":"20251012165751-7k38r6a","updated":"20251012231145"},"Children":[{"Type":"NodeText","Data":"除以dk能缩小点积规模，让softmax输出更平缓，保证梯度正常传递，结果归一化成均值为0、方差为1的向量（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-font-background8); color: var(--b3-font-color7);"},"TextMarkType":"strong mark","TextMarkTextContent":"让前后分布均值方差一致"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-font-background8); color: var(--b3-font-color7);\"}"},{"Type":"NodeText","Data":"），避免梯度消失。"}]}]}]}]},{"ID":"20251012165751-cpbj60e","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012165751-cpbj60e","updated":"20251012170530"},"Children":[{"ID":"20251012165751-ecu0nwh","Type":"NodeParagraph","Properties":{"id":"20251012165751-ecu0nwh","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"为什么是根号："}]},{"ID":"20251012165751-0yl9lx3","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012165751-0yl9lx3","updated":"20251012170530"},"Children":[{"ID":"20251012165751-0eze3lc","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-0eze3lc","updated":"20251012165751"},"Children":[{"ID":"20251012165751-bng9hvc","Type":"NodeParagraph","Properties":{"id":"20251012165751-bng9hvc","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"q和k向量里的每一个变量是0 1分布，那么根据变量乘积的方差公式 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" var(qk)＝var(q)var(k)+var(q)E(k)^2+var(k)E(q)^2"},{"Type":"NodeText","Data":" ，E(q)和E(k)都是0，那么就是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"var(qk)＝var(q)var(k)"},{"Type":"NodeText","Data":"，也就是qk每个变量乘积之后方差是1，qk点积的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"方差"},{"Type":"NodeText","Data":"就是dk"}]}]},{"ID":"20251012165751-7c2au37","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012165751-7c2au37","updated":"20251012170530"},"Children":[{"ID":"20251012165751-hdqpg62","Type":"NodeParagraph","Properties":{"id":"20251012165751-hdqpg62","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"方差是dk，方差是平方，所以除以根号dk"}]},{"ID":"20251012165751-im2htrn","Type":"NodeParagraph","Properties":{"id":"20251012165751-im2htrn","updated":"20251012170413"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Properties":{"style":"width: 621px;"},"Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012165748.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"width: 621px;\"}"},{"Type":"NodeText","Data":"​"}]},{"ID":"20251012170524-izhc5w0","Type":"NodeParagraph","Properties":{"id":"20251012170524-izhc5w0","updated":"20251012170530"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Properties":{"style":"width: 603px;"},"Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012170526.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"width: 603px;\"}"},{"Type":"NodeText","Data":"​"}]}]}]}]},{"ID":"20251012165751-hhyp6jg","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251012165751-hhyp6jg","updated":"20251012170736"},"Children":[{"ID":"20251012165751-om1x2kl","Type":"NodeParagraph","Properties":{"id":"20251012165751-om1x2kl","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"能换成别的数吗？"}]},{"ID":"20251012165751-7009y4j","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012165751-7009y4j","updated":"20251012170736"},"Children":[{"ID":"20251012165751-dzygz4n","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-dzygz4n","updated":"20251012170736"},"Children":[{"ID":"20251012165751-8x8u94k","Type":"NodeParagraph","Properties":{"id":"20251012165751-8x8u94k","updated":"20251012170736"},"Children":[{"Type":"NodeText","Data":"根号dk能够让数据方差分布更好"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012161559-0gkmch8","Type":"NodeParagraph","Properties":{"id":"20251012161559-0gkmch8","updated":"20251012161559"}},{"ID":"20251012162459-d2syp3v","Type":"NodeParagraph","Properties":{"id":"20251012162459-d2syp3v","updated":"20251012162459"}},{"ID":"20251012231203-0w5fa93","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231203-0w5fa93","updated":"20251012231237"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012161559-sq60ktt","Type":"NodeParagraph","Properties":{"id":"20251012161559-sq60ktt","updated":"20251012170822"},"Children":[{"Type":"NodeText","Data":"Attention的时间复杂度是多少？"}]},{"ID":"20251012171821-1wwfn0i","Type":"NodeBlockquote","Properties":{"id":"20251012171821-1wwfn0i","updated":"20251012231228"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012171822-9a1sg0f","Type":"NodeParagraph","Properties":{"id":"20251012171822-9a1sg0f","updated":"20251012231228"},"Children":[{"Type":"NodeText","Data":"根据矩阵乘法规则：m×k 矩阵与 k×p 矩阵相乘，时间复杂度是"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"m（行）×k（矩阵相交）×p（列）"}]}]},{"ID":"20251012170822-h61bs4a","Type":"NodeList","ListData":{},"Properties":{"id":"20251012170822-h61bs4a","updated":"20251012231237"},"Children":[{"ID":"20251012170852-8yrouco","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012170852-8yrouco","updated":"20251012231237"},"Children":[{"ID":"20251012170852-tl4zxwm","Type":"NodeParagraph","Properties":{"id":"20251012170852-tl4zxwm","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"输入序列长度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"，特征维度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"。总时间复杂度为"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"mark"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"inline-math mark","TextMarkInlineMathContent":"O(nd^2)(\\text{线性变换})+O(n^2d)(\\text{点积与加权和})+O(n^2)(\\mathrm{softmax})"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012170920-pcv2jhk","Type":"NodeList","ListData":{},"Properties":{"id":"20251012170920-pcv2jhk","updated":"20251012183020"},"Children":[{"ID":"20251012170919-l3knork","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012170919-l3knork","updated":"20251012170919"},"Children":[{"ID":"20251012170919-993idfu","Type":"NodeParagraph","Properties":{"id":"20251012170919-993idfu","updated":"20251012171247"},"Children":[{"Type":"NodeText","Data":"第一步 QKV的线性变换：计算 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"XW^Q、XW^V、XW^K"},{"Type":"NodeText","Data":"，每个token计算一次，计算n次，矩阵计算两个for循环，长度为d，所以为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n·d^2)"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012171518-vidy11o","Type":"NodeList","ListData":{},"Properties":{"id":"20251012171518-vidy11o","updated":"20251012171518"},"Children":[{"ID":"20251012171518-i9mty7h","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171518-i9mty7h","updated":"20251012171518"},"Children":[{"ID":"20251012171518-zo0ovnj","Type":"NodeParagraph","Properties":{"id":"20251012171518-zo0ovnj","updated":"20251012171840"},"Children":[{"Type":"NodeText","Data":"Q 的形状是 n×d（n 个 token，每个 d 维）"}]}]}]}]},{"ID":"20251012171248-isvr0wq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171248-isvr0wq","updated":"20251012171248"},"Children":[{"ID":"20251012171248-k7mqk6g","Type":"NodeParagraph","Properties":{"id":"20251012171248-k7mqk6g","updated":"20251012171724"},"Children":[{"Type":"NodeText","Data":"第二步 计算注意力分数：计算"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"QK^T_esc_newline_"},{"Type":"NodeText","Data":"，n×d与d×n的矩阵相乘，时间复杂度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n^2·d)"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012171901-ryjktg5","Type":"NodeList","ListData":{},"Properties":{"id":"20251012171901-ryjktg5","updated":"20251012171901"},"Children":[{"ID":"20251012171901-e9vn30u","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171901-e9vn30u","updated":"20251012171901"},"Children":[{"ID":"20251012171901-3swja8i","Type":"NodeParagraph","Properties":{"id":"20251012171901-3swja8i","updated":"20251012171924"},"Children":[{"Type":"NodeText","Data":"这一步是瓶颈，因为n平方，随着序列长度增长会变大"}]}]},{"ID":"20251012172028-et75x7k","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172028-et75x7k","updated":"20251012172028"},"Children":[{"ID":"20251012172028-yq64bni","Type":"NodeParagraph","Properties":{"id":"20251012172028-yq64bni","updated":"20251012172035"},"Children":[{"Type":"NodeText","Data":"输出结果为n×n"}]}]}]}]},{"ID":"20251012171924-7j7jgzs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171924-7j7jgzs","updated":"20251012172101"},"Children":[{"ID":"20251012171924-94zb6dc","Type":"NodeParagraph","Properties":{"id":"20251012171924-94zb6dc","updated":"20251012172001"},"Children":[{"Type":"NodeText","Data":"第三步 softmax： "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n^2)"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012172101-2mq9626","Type":"NodeList","ListData":{},"Properties":{"id":"20251012172101-2mq9626","updated":"20251012172101"},"Children":[{"ID":"20251012172101-hdsymql","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172101-hdsymql","updated":"20251012172101"},"Children":[{"ID":"20251012172101-dvk7gik","Type":"NodeParagraph","Properties":{"id":"20251012172101-dvk7gik","updated":"20251012172101"},"Children":[{"Type":"NodeText","Data":"第一步：对该行每个元素求指数（exp），共 n 次操作；"}]}]},{"ID":"20251012172101-5wjipka","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172101-5wjipka","updated":"20251012172101"},"Children":[{"ID":"20251012172101-3im0ihy","Type":"NodeParagraph","Properties":{"id":"20251012172101-3im0ihy","updated":"20251012172101"},"Children":[{"Type":"NodeText","Data":"第二步：计算该行所有元素的 exp 和（分母），再用每个元素的 exp 除以这个和，又需 n 次操作。\n每行总操作数为 n + n "},{"Type":"NodeBackslash","Data":"span","Children":[{"Type":"NodeText","Data":"="}]},{"Type":"NodeText","Data":" 2n，n 行则总操作数为 n×2n "},{"Type":"NodeBackslash","Data":"span","Children":[{"Type":"NodeText","Data":"="}]},{"Type":"NodeText","Data":" O (n²)（常数 2 可忽略，复杂度取主导项）。"}]}]}]}]},{"ID":"20251012172124-i9bla01","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172124-i9bla01","updated":"20251012183020"},"Children":[{"ID":"20251012172124-63aozch","Type":"NodeParagraph","Properties":{"id":"20251012172124-63aozch","updated":"20251012172129"},"Children":[{"Type":"NodeText","Data":"添加上V："}]},{"ID":"20251012182903-dh1qqk7","Type":"NodeList","ListData":{},"Properties":{"id":"20251012182903-dh1qqk7","updated":"20251012183020"},"Children":[{"ID":"20251012182902-ub7x1or","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012182902-ub7x1or","updated":"20251012183020"},"Children":[{"ID":"20251012182902-0gfe25e","Type":"NodeParagraph","Properties":{"id":"20251012182902-0gfe25e","updated":"20251012183020"},"Children":[{"Type":"NodeText","Data":"计算"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"softmax(Q·KT)·V"},{"Type":"NodeText","Data":"的复杂度是"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n^2·d)"},{"Type":"NodeText","Data":"。因为每个注意力权重需要乘以对应的Value向量。"}]}]}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251009171816-5cabewt","Type":"NodeParagraph","Properties":{"id":"20251009171816-5cabewt","updated":"20251009171816"}},{"ID":"20251012162711-m1fjyva","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20251012162711-m1fjyva","updated":"20251019172705"},"Children":[{"Type":"NodeText","Data":"多头注意力"}]},{"ID":"20251012231305-qmyy076","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231305-qmyy076","updated":"20251012231346"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012183148-l14zjsp","Type":"NodeParagraph","Properties":{"id":"20251012183148-l14zjsp","updated":"20251012231305"},"Children":[{"Type":"NodeText","Data":"Transformer为何使用多头注意力机制？（为什么不使用一个头）"}]},{"ID":"20251012183309-f2ib6xh","Type":"NodeList","ListData":{},"Properties":{"id":"20251012183309-f2ib6xh","updated":"20251012231346"},"Children":[{"ID":"20251012184053-mvx0jve","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184053-mvx0jve","updated":"20251012231320"},"Children":[{"ID":"20251012184053-770zit6","Type":"NodeParagraph","Properties":{"id":"20251012184053-770zit6","updated":"20251012231320"},"Children":[{"Type":"NodeText","Data":"核心思想：将Q,K,V通过"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"不同的线性变换"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"（投影）映射到"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"多个不同的子空间"},{"Type":"NodeText","Data":"(subspace)中，在每个子空间里"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"独立地"},{"Type":"NodeText","Data":"执行缩放点积注意力(Scaled Dot-Product Attention),最后将所有"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"子空间的注意力输出拼接起来"},{"Type":"NodeText","Data":"，再进行一次线性变换得到最终的输出。"}]}]},{"ID":"20251012184106-k7dw8gh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184106-k7dw8gh","updated":"20251012231346"},"Children":[{"ID":"20251012184106-wpk0izl","Type":"NodeParagraph","Properties":{"id":"20251012184106-wpk0izl","updated":"20251012184106"},"Children":[{"Type":"NodeText","Data":"作用与好处："}]},{"ID":"20251012184109-qonmnlk","Type":"NodeList","ListData":{},"Properties":{"id":"20251012184109-qonmnlk","updated":"20251012231346"},"Children":[{"ID":"20251012184109-ig40dpl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184109-ig40dpl","updated":"20251012231338"},"Children":[{"ID":"20251012184109-0nji5ko","Type":"NodeParagraph","Properties":{"id":"20251012184109-0nji5ko","updated":"20251012231329"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"允许模型关注来自不同子空间的不同信息："},{"Type":"NodeText","Data":"这是最关键的作用。"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text","TextMarkTextContent":"单头注意力机制可能会让模型只关注"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text mark","TextMarkTextContent":"某一种特定类型"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text","TextMarkTextContent":"的关联信息（比如语法关系），或者所有"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text mark","TextMarkTextContent":"信息被平均化"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeText","Data":"。多头机制允许不同的\"头”\"(head)学习关注输入序列中不同方面的信息。"}]},{"ID":"20251012184457-adyy54b","Type":"NodeList","ListData":{},"Properties":{"id":"20251012184457-adyy54b","updated":"20251012231338"},"Children":[{"ID":"20251012184207-pxrx6ae","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184207-pxrx6ae","updated":"20251012184510"},"Children":[{"ID":"20251012184207-8aeozsg","Type":"NodeBlockquote","Properties":{"id":"20251012184207-8aeozsg","updated":"20251012184207"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012184207-cwpbybp","Type":"NodeParagraph","Properties":{"id":"20251012184207-cwpbybp","updated":"20251012184207"},"Children":[{"Type":"NodeText","Data":"例如，一个头可能关注局部的语法依赖，另一个头可能关注长距离的语义关联，还有一个头可能关注词语的位置信息等。这使得模型能够更全面、更细致地捕捉输入信息中丰富的特征和依赖关系。"}]}]}]},{"ID":"20251012184607-6cuz29n","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184607-6cuz29n","updated":"20251012231334"},"Children":[{"ID":"20251012184607-ryg70rv","Type":"NodeParagraph","Properties":{"id":"20251012184607-ryg70rv","updated":"20251012231334"},"Children":[{"Type":"NodeText","Data":"提供更丰富的表示能力：每个头可以看作是在"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"不同的表示视角"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"下进行注意力计算"},{"Type":"NodeText","Data":"。将这些不同视角的结果结合起来，可以产生比单视角更强大、更鲁棒的特征表示。"}]}]},{"ID":"20251012184636-vnydbb8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184636-vnydbb8","updated":"20251012231338"},"Children":[{"ID":"20251012184636-1mk670o","Type":"NodeParagraph","Properties":{"id":"20251012184636-1mk670o","updated":"20251012231338"},"Children":[{"Type":"NodeText","Data":"类似集成学习的效果：在某种程度上，多头注意力有点像"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"集成学习(Ensemble Learning)"},{"Type":"NodeText","Data":"的思想，不同的头可以看作是不同的“专家”，各自专注于序列的不同方面，最后综合它们的意见，得到更优的整体判断"}]}]}]}]},{"ID":"20251012184640-3x5yzlw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184640-3x5yzlw","updated":"20251012231346"},"Children":[{"ID":"20251012184640-sf3chef","Type":"NodeParagraph","Properties":{"id":"20251012184640-sf3chef","updated":"20251012231346"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"稳定训练过程（间接作用）"},{"Type":"NodeText","Data":"：将"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"原始的高维空间分解为多个低维子空间"},{"Type":"NodeText","Data":"进行计算，可能有助于稳定训练，降低模型对某些特定模式过度敏感的风险。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012184655-4ci9w2m","Type":"NodeParagraph","Properties":{"id":"20251012184655-4ci9w2m","updated":"20251012184655"}},{"ID":"20251012184655-aej1wih","Type":"NodeParagraph","Properties":{"id":"20251012184655-aej1wih","updated":"20251012184655"}},{"ID":"20251012231359-6dlt12t","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231359-6dlt12t","updated":"20251012231410"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012183309-0m2kl6c","Type":"NodeParagraph","Properties":{"id":"20251012183309-0m2kl6c","updated":"20251012231400"},"Children":[{"Type":"NodeText","Data":"多头注意力的时空复杂度？"}]},{"ID":"20251012223353-nu9nzrt","Type":"NodeBlockquote","Properties":{"id":"20251012223353-nu9nzrt","updated":"20251012231400"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012223354-2fskso4","Type":"NodeParagraph","Properties":{"id":"20251012223354-2fskso4","updated":"20251012223437"},"Children":[{"Type":"NodeText","Data":"相似问题："},{"Type":"NodeTextMark","TextMarkType":"block-ref","TextMarkBlockRefID":"20251012161559-sq60ktt","TextMarkBlockRefSubtype":"d","TextMarkTextContent":"Attention的时间复杂度是多少？"}]}]},{"ID":"20251012223304-btral2z","Type":"NodeParagraph","Properties":{"id":"20251012223304-btral2z","updated":"20251012231400"},"Children":[{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"个输入向量的序列，每个向量的维度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"d"}]},{"ID":"20251012183319-ipu614d","Type":"NodeList","ListData":{},"Properties":{"id":"20251012183319-ipu614d","updated":"20251012231410"},"Children":[{"ID":"20251012223243-inhesy9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012223243-inhesy9","updated":"20251012231410"},"Children":[{"ID":"20251012223243-t5j2h2s","Type":"NodeParagraph","Properties":{"id":"20251012223243-t5j2h2s","updated":"20251012223246"},"Children":[{"Type":"NodeText","Data":"时间复杂度："}]},{"ID":"20251012223247-g2xdzt6","Type":"NodeList","ListData":{},"Properties":{"id":"20251012223247-g2xdzt6","updated":"20251012231410"},"Children":[{"ID":"20251012223351-t2m1cxz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012223351-t2m1cxz","updated":"20251012231410"},"Children":[{"ID":"20251012223351-lzlzlm2","Type":"NodeParagraph","Properties":{"id":"20251012223351-lzlzlm2","updated":"20251012231410"},"Children":[{"Type":"NodeText","Data":"多头计算：如果使用个头的多头注意力机制，那么需要对每个头分别进行上述计算，因此总的时间复杂度"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"为"},{"Type":"NodeTextMark","TextMarkType":"inline-math mark","TextMarkInlineMathContent":"O(hn^2d)"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"。"}]}]}]}]}]},{"ID":"20251012183126-skz0338","Type":"NodeParagraph","Properties":{"id":"20251012183126-skz0338","style":"background-color: var(--b3-font-background1); --b3-parent-background: var(--b3-font-background1);","updated":"20251012231400"},"Children":[{"Type":"NodeText","Data":"虽然每个头都独立地计算注意力权重，但是这些计算可以在现代硬件（如GPU）上并行执行，提高了计算效率。"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012224145-gjp5tpv","Type":"NodeParagraph","Properties":{"id":"20251012224145-gjp5tpv","updated":"20251012224145"}},{"ID":"20251019172703-6jnnwsl","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251019172703-6jnnwsl","updated":"20251019172748"},"Children":[{"Type":"NodeText","Data":"FNN部分"}]},{"ID":"20251019172709-i2glhvq","Type":"NodeParagraph","Properties":{"id":"20251019172709-i2glhvq","updated":"20251019172727"},"Children":[{"Type":"NodeText","Data":"为什么transformer的FFN需要先升维再降维"}]},{"ID":"20251019172728-i4cc9xl","Type":"NodeParagraph","Properties":{"id":"20251019172728-i4cc9xl","updated":"20251019172741"},"Children":[{"Type":"NodeText","Data":"FFN通常把输入从512维扩展到2048维，再投影回512维。"}]},{"ID":"20251019172748-mcz5ryi","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172748-mcz5ryi","updated":"20251019172748"},"Children":[{"ID":"20251019172748-3j3a9fv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172748-3j3a9fv","updated":"20251019172748"},"Children":[{"ID":"20251019172748-ofonlgb","Type":"NodeParagraph","Properties":{"id":"20251019172748-ofonlgb","updated":"20251019172806"},"Children":[{"Type":"NodeText","Data":"同维度表达有上限："}]},{"ID":"20251019172807-vkmflkj","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172807-vkmflkj","updated":"20251019172807"},"Children":[{"ID":"20251019172807-ntzu5vj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172807-ntzu5vj","updated":"20251019172807"},"Children":[{"ID":"20251019172807-m4wqms9","Type":"NodeParagraph","Properties":{"id":"20251019172807-m4wqms9","updated":"20251019172823"},"Children":[{"Type":"NodeText","Data":"直接用512→512的线性层，本质上就是矩阵乘法，无论叠多少层都等价于"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"一个线性变换"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20251019172811-7k5ndtd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172811-7k5ndtd","updated":"20251019172811"},"Children":[{"ID":"20251019172811-bvh5luf","Type":"NodeParagraph","Properties":{"id":"20251019172811-bvh5luf","updated":"20251019172828"},"Children":[{"Type":"NodeText","Data":"加了激活函数后确实引入了非线性，但在同维度空间里，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"特征的变换能力"},{"Type":"NodeText","Data":"是有上限的。"}]}]},{"ID":"20251019172829-0ew7kim","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172829-0ew7kim","updated":"20251019172829"},"Children":[{"ID":"20251019172829-ifndvit","Type":"NodeParagraph","Properties":{"id":"20251019172829-ifndvit","updated":"20251019172841"},"Children":[{"Type":"NodeText","Data":"升到2048维后，激活函数能在"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"更高维度的空间里切分数据"},{"Type":"NodeText","Data":"，创造出原本512维空间里不存在的"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"特征组合"},{"Type":"NodeText","Data":"。降维过程则是把高维空间学到的复杂特征"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"映射回原始维度"},{"Type":"NodeText","Data":"，相当于做了一次信息提炼。"}]}]}]}]},{"ID":"20251019172844-b8tnq1f","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172844-b8tnq1f","updated":"20251019172844"},"Children":[{"ID":"20251019172844-sxm90pw","Type":"NodeParagraph","Properties":{"id":"20251019172844-sxm90pw","updated":"20251019172851"},"Children":[{"Type":"NodeText","Data":"高维切分更复杂："}]},{"ID":"20251019172852-2jarmy6","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172852-2jarmy6","updated":"20251019172852"},"Children":[{"ID":"20251019172851-0ytylvm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172851-0ytylvm","updated":"20251019172851"},"Children":[{"ID":"20251019172851-hx7v6ye","Type":"NodeParagraph","Properties":{"id":"20251019172851-hx7v6ye","updated":"20251019172922"},"Children":[{"Type":"NodeText","Data":"ReLU的本质是分段线性函数，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"每个神经元对应一个超平面"},{"Type":"NodeText","Data":"。512维空间只能用512个超平面切分，能表示的"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"函数复杂度"},{"Type":"NodeText","Data":"有限。"}]}]},{"ID":"20251019172909-yab91w0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172909-yab91w0","updated":"20251019172909"},"Children":[{"ID":"20251019172909-7ezxykh","Type":"NodeParagraph","Properties":{"id":"20251019172909-7ezxykh","updated":"20251019172909"},"Children":[{"Type":"NodeText","Data":"升到2048维后，有2048个超平面参与切分，能构建的分段线性区域数量呈指数级增长。这些高维区域对应着不同的语义模式，降维时这些复杂的高维模式被线性组合投影回512维，保留了关键信息但维度回到了原点"}]}]}]}]},{"ID":"20251019172954-qcmfrwy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172954-qcmfrwy","updated":"20251019172954"},"Children":[{"ID":"20251019172954-kgka122","Type":"NodeParagraph","Properties":{"id":"20251019172954-kgka122","updated":"20251019173010"},"Children":[{"Type":"NodeText","Data":"参数量决定容量："}]},{"ID":"20251019173011-uxzzy4h","Type":"NodeList","ListData":{},"Properties":{"id":"20251019173011-uxzzy4h","updated":"20251019173011"},"Children":[{"ID":"20251019173011-dw4tzbo","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019173011-dw4tzbo","updated":"20251019173011"},"Children":[{"ID":"20251019173011-lh8e25e","Type":"NodeParagraph","Properties":{"id":"20251019173011-lh8e25e","updated":"20251019173039"},"Children":[{"Type":"NodeText","Data":"神经网络的参数本质上在存储"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"输入输出的映射"},{"Type":"NodeText","Data":"关系。"}]}]},{"ID":"20251019173034-tpng4gn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019173034-tpng4gn","updated":"20251019173034"},"Children":[{"ID":"20251019173034-obaylcm","Type":"NodeParagraph","Properties":{"id":"20251019173034-obaylcm","updated":"20251019173049"},"Children":[{"Type":"NodeText","Data":"512→512的FFN只有512²≈26万参数，512→2048→512有约210万参数，足足8倍的差距。这些参数可以看成key-value记忆，第一层学习\""},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"什么样的输入模式应该被识别"},{"Type":"NodeText","Data":"\"，第二层学习\""},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"识别到这个模式后应该输出什么"},{"Type":"NodeText","Data":"\"。参数量越大，能记住的模式越多"}]}]}]}]},{"ID":"20251019173051-l2gu0za","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019173051-l2gu0za","updated":"20251019173051"},"Children":[{"ID":"20251019173051-7og2e83","Type":"NodeParagraph","Properties":{"id":"20251019173051-7og2e83","updated":"20251019173051"}}]}]},{"ID":"20251019172703-ia55pty","Type":"NodeParagraph","Properties":{"id":"20251019172703-ia55pty","updated":"20251019172703"}},{"ID":"20251012183126-4a80w3s","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251012183126-4a80w3s","updated":"20251019172646"},"Children":[{"Type":"NodeText","Data":"Transformer"}]},{"ID":"20251012231421-ikdm4ag","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231421-ikdm4ag","updated":"20251012231506"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012184805-ir1cpk8","Type":"NodeParagraph","Properties":{"id":"20251012184805-ir1cpk8","updated":"20251012231422"},"Children":[{"Type":"NodeText","Data":"Transformer多层的作用？"}]},{"ID":"20251012184817-kk3ap43","Type":"NodeBlockquote","Properties":{"id":"20251012184817-kk3ap43","updated":"20251012231422"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012184817-zo96whp","Type":"NodeParagraph","Properties":{"id":"20251012184817-zo96whp","updated":"20251012185058"},"Children":[{"Type":"NodeText","Data":" 相似问题： "},{"Type":"NodeTextMark","TextMarkType":"block-ref","TextMarkBlockRefID":"20251012183148-l14zjsp","TextMarkBlockRefSubtype":"d","TextMarkTextContent":"Transformer为何使用多头注意力机制？（为什么不使用一个头）"}]}]},{"ID":"20251012184754-s0mdwx9","Type":"NodeList","ListData":{},"Properties":{"id":"20251012184754-s0mdwx9","updated":"20251012231506"},"Children":[{"ID":"20251012185148-3vjatpq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185148-3vjatpq","updated":"20251012185148"},"Children":[{"ID":"20251012185148-vcymdhq","Type":"NodeParagraph","Properties":{"id":"20251012185148-vcymdhq","updated":"20251012185214"},"Children":[{"Type":"NodeText","Data":"核心思想：transfomer将基础的"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"\"块\"(Block/Layer)"},{"Type":"NodeText","Data":"​堆叠多次，形成一个深层网络结构。每一层的输出作为下一层的输入。"}]}]},{"ID":"20251012185218-cowl9jm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185218-cowl9jm","updated":"20251012231506"},"Children":[{"ID":"20251012185218-3d1ql0n","Type":"NodeParagraph","Properties":{"id":"20251012185218-3d1ql0n","updated":"20251012185221"},"Children":[{"Type":"NodeText","Data":"作用与好处："}]},{"ID":"20251012185221-gcvjzvx","Type":"NodeList","ListData":{},"Properties":{"id":"20251012185221-gcvjzvx","updated":"20251012231506"},"Children":[{"ID":"20251012185221-ts8z3jn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185221-ts8z3jn","updated":"20251012231436"},"Children":[{"ID":"20251012185221-w1e84ju","Type":"NodeParagraph","Properties":{"id":"20251012185221-w1e84ju","updated":"20251012231428"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"逐步提取和组合更复杂的特征"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"："},{"Type":"NodeText","Data":"模型进行层次化的信息处理"}]},{"ID":"20251012185235-18yz54p","Type":"NodeList","ListData":{},"Properties":{"id":"20251012185235-18yz54p","updated":"20251012231436"},"Children":[{"ID":"20251012185234-oeh3059","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185234-oeh3059","updated":"20251012231431"},"Children":[{"ID":"20251012185234-6skpbw7","Type":"NodeParagraph","Properties":{"id":"20251012185234-6skpbw7","updated":"20251012231431"},"Children":[{"Type":"NodeText","Data":"底层（靠近输入的层）:可能更关注"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"词语本身、局部的上下文"},{"Type":"NodeText","Data":"依赖关系（如短语结构）"}]}]},{"ID":"20251012185237-3lb4df0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185237-3lb4df0","updated":"20251012231434"},"Children":[{"ID":"20251012185237-0qqf99t","Type":"NodeParagraph","Properties":{"id":"20251012185237-0qqf99t","updated":"20251012231434"},"Children":[{"Type":"NodeText","Data":"中层可以在底层的基础上：开始"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"捕捉更长距离的依赖、句法结构"},{"Type":"NodeText","Data":"或简单的语义关系"}]}]},{"ID":"20251012185255-eyukcyp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185255-eyukcyp","updated":"20251012231436"},"Children":[{"ID":"20251012185255-wrc42ic","Type":"NodeParagraph","Properties":{"id":"20251012185255-wrc42ic","updated":"20251012231436"},"Children":[{"Type":"NodeText","Data":"高层（靠近输出的层）则可以整合来自中下层的信息，进行更"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"复杂的推理"},{"Type":"NodeText","Data":"，理解更高层次的语义、语篇结构、甚至是抽象概念之间的联系"}]}]}]}]},{"ID":"20251012185303-vfp3fjz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185303-vfp3fjz","updated":"20251012231459"},"Children":[{"ID":"20251012185303-wqbm9rn","Type":"NodeParagraph","Properties":{"id":"20251012185303-wqbm9rn","updated":"20251012231455"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"增加模型容量和表达能力："},{"Type":"NodeText","Data":"网络的深度是"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"模型复杂度和表达能力"},{"Type":"NodeText","Data":"的关键因素。"}]},{"ID":"20251012185322-ed3gvjm","Type":"NodeList","ListData":{},"Properties":{"id":"20251012185322-ed3gvjm","updated":"20251012231459"},"Children":[{"ID":"20251012185322-lkgzewx","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185322-lkgzewx","updated":"20251012231459"},"Children":[{"ID":"20251012185322-coqpnjm","Type":"NodeParagraph","Properties":{"id":"20251012185322-coqpnjm","updated":"20251012231459"},"Children":[{"Type":"NodeText","Data":"更多的层意味着模型"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"拥有更多的参数和非线性变换"},{"Type":"NodeText","Data":"，能够学习和拟合更复杂的数据模式和函数。"}]}]},{"ID":"20251012185325-x0qp4sf","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185325-x0qp4sf","updated":"20251012185325"},"Children":[{"ID":"20251012185325-ruyrlgy","Type":"NodeParagraph","Properties":{"id":"20251012185325-ruyrlgy","updated":"20251012185327"},"Children":[{"Type":"NodeText","Data":"对于需要理解复杂语言现象的任务(如机器翻译、文本生成、问答)，深层结构是必不可少的"}]}]}]}]},{"ID":"20251012185328-ub9pltc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185328-ub9pltc","updated":"20251012231506"},"Children":[{"ID":"20251012185328-0w38rzi","Type":"NodeParagraph","Properties":{"id":"20251012185328-0w38rzi","updated":"20251012231506"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"逐层精炼："},{"Type":"NodeText","Data":"每一层都可以看作是"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"对上一层输出表示的一次\"精炼”"},{"Type":"NodeText","Data":"(refinement)。通过多层堆叠，信息在网络中逐层传播和处理，使得最"},{"Type":"NodeTextMark","TextMarkType":"u","TextMarkTextContent":"终的表示能够充分融入上下文信息"},{"Type":"NodeText","Data":"，并达到"},{"Type":"NodeTextMark","TextMarkType":"u","TextMarkTextContent":"较高的抽象"},{"Type":"NodeText","Data":"水平。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012184754-b56aqxw","Type":"NodeParagraph","Properties":{"id":"20251012184754-b56aqxw","updated":"20251012184754"}},{"ID":"20251019172646-epcj8qn","Type":"NodeParagraph","Properties":{"id":"20251019172646-epcj8qn","updated":"20251019172646"}},{"ID":"20251019172646-m1ipi6y","Type":"NodeParagraph","Properties":{"id":"20251019172646-m1ipi6y","updated":"20251019172646"}},{"ID":"20251019172646-n55x4my","Type":"NodeParagraph","Properties":{"id":"20251019172646-n55x4my","updated":"20251019172646"}},{"ID":"20251012162711-qdpdez2","Type":"NodeParagraph","Properties":{"id":"20251012162711-qdpdez2","updated":"20251012162712"},"Children":[{"Type":"NodeText","Data":"什么因素会导致模型训练不稳定，什么激活函数会导致训练不稳定，什么原因会导致梯度消失/爆炸"}]},{"ID":"20251012163359-youj204","Type":"NodeParagraph","Properties":{"id":"20251012163359-youj204","updated":"20251012163359"}},{"ID":"20251012163400-xldb6l5","Type":"NodeParagraph","Properties":{"id":"20251012163400-xldb6l5","updated":"20251012163400"}},{"ID":"20251012163526-pw5ab6n","Type":"NodeParagraph","Properties":{"id":"20251012163526-pw5ab6n","updated":"20251012163529"},"Children":[{"Type":"NodeText","Data":"1.Transformer为何使用多头注意力机制？（为什么不使用一个头）\n2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？ （注意和第一个问题的区别）\n3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n5.在计算attention score的时候如何对padding做mask操作？\n6.为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n7.大概讲一下Transformer的Encoder模块？\n8.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n9.简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n10.你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n11.简单讲一下Transformer中的残差结构以及意义。\n12.为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n13.简答讲一下BatchNorm技术，以及它的优缺点。\n14.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\n15.Encoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\n16.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\n17.Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n19.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n20解码端的残差结构有没有把后续未被看见的mask信息添加进来，造成信息的泄露。"}]},{"ID":"20251012163400-3f1dued","Type":"NodeParagraph","Properties":{"id":"20251012163400-3f1dued","updated":"20251015232251"},"Children":[{"Type":"NodeText","Data":"llm计算loss为什么用交叉熵而不是mse？"}]},{"ID":"20251012163400-5894i0x","Type":"NodeParagraph","Properties":{"id":"20251012163400-5894i0x","updated":"20251012163400"}}]}