{"ID":"20251009102118-ov7p86j","Spec":"1","Type":"NodeDocument","Properties":{"icon":"1f427","id":"20251009102118-ov7p86j","title":"八股","type":"doc","updated":"20251012164215"},"Children":[{"ID":"20251009102119-bk79nfo","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20251009102119-bk79nfo","updated":"20251012164215"},"Children":[{"Type":"NodeText","Data":"大模型基础"}]},{"ID":"20251009153935-3my4s8g","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20251009153935-3my4s8g","updated":"20251012164215"},"Children":[{"Type":"NodeText","Data":"机制部分"}]},{"ID":"20251009153946-a1q9qdf","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251009153946-a1q9qdf","updated":"20251012164215"},"Children":[{"Type":"NodeText","Data":"注意力"}]},{"ID":"20251009153412-yykcka0","Type":"NodeMathBlock","Properties":{"id":"20251009153412-yykcka0","updated":"20251009153923"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20251009153923-8je37xh","Type":"NodeParagraph","Properties":{"id":"20251009153923-8je37xh","updated":"20251009171601"},"Children":[{"Type":"NodeText","Data":"注意力的过程?"}]},{"ID":"20251009154203-tuvpfcq","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251009154203-tuvpfcq","updated":"20251009172100"},"Children":[{"ID":"20251009154205-tkdo68p","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251009154205-tkdo68p","updated":"20251009154556"},"Children":[{"ID":"20251009154205-fglxmb0","Type":"NodeParagraph","Properties":{"id":"20251009154205-fglxmb0","updated":"20251009154556"},"Children":[{"Type":"NodeText","Data":"对于输入序列的每个位置，通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"点积"},{"Type":"NodeText","Data":"计算其与其他位置之间的相似度得分"}]}]},{"ID":"20251009154236-1a0l4zk","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251009154236-1a0l4zk","updated":"20251009172100"},"Children":[{"ID":"20251009154236-cvyxp3j","Type":"NodeParagraph","Properties":{"id":"20251009154236-cvyxp3j","updated":"20251009172100"},"Children":[{"Type":"NodeText","Data":"然后对得分进行放缩处理，以防止梯度消失"}]}]},{"ID":"20251009171345-m7fwpby","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251009171345-m7fwpby","updated":"20251009171426"},"Children":[{"ID":"20251009171345-80siu0h","Type":"NodeParagraph","Properties":{"id":"20251009171345-80siu0h","updated":"20251009171426"},"Children":[{"Type":"NodeText","Data":"将得分用"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"softmax"},{"Type":"NodeText","Data":"​函数转换为注意力权重，以便计算每个位置的加权和"}]}]}]},{"ID":"20251009154247-j478x3c","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251009154247-j478x3c","updated":"20251009171533"},"Children":[{"ID":"20251009171532-qzorkai","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20251009171532-qzorkai","updated":"20251009171533"},"Children":[{"ID":"20251009171532-jkf4kl4","Type":"NodeParagraph","Properties":{"id":"20251009171532-jkf4kl4","updated":"20251009171533"},"Children":[{"Type":"NodeText","Data":"使用注意力权重对输入序列中的所有位置进行加权求和，得到每个位置的自注意输出。"}]}]}]},{"ID":"20251009154247-r0cts7t","Type":"NodeParagraph","Properties":{"id":"20251009154247-r0cts7t","updated":"20251009154247"}},{"ID":"20251009154247-77nyjqd","Type":"NodeParagraph","Properties":{"id":"20251009154247-77nyjqd","updated":"20251012163704"},"Children":[{"Type":"NodeText","Data":"为什么要除以根号dk？能换成别的数吗？"}]},{"ID":"20251012162135-qza7x2g","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012162135-qza7x2g","updated":"20251012164215"},"Children":[{"ID":"20251012162135-m9d37ip","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012162135-m9d37ip","updated":"20251012163656"},"Children":[{"ID":"20251012162135-49g3z9i","Type":"NodeParagraph","Properties":{"id":"20251012162135-49g3z9i","updated":"20251012164006"},"Children":[{"Type":"NodeText","Data":"避免softmax梯度消失："}]},{"ID":"20251012164025-hq4wmm2","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012164025-hq4wmm2","updated":"20251012164025"},"Children":[{"ID":"20251012164025-xw7hc60","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012164025-xw7hc60","updated":"20251012164025"},"Children":[{"ID":"20251012164025-r8o60tb","Type":"NodeParagraph","Properties":{"id":"20251012164025-r8o60tb","updated":"20251012164032"},"Children":[{"Type":"NodeText","Data":"当dk（Query/Key的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"维度"},{"Type":"NodeText","Data":"）"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"较大"},{"Type":"NodeText","Data":"时，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Q和K的点积结果会随dk增大而显著变大 （"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);"},"TextMarkType":"em text","TextMarkTextContent":"根号dk越大，维度变多，累加变多，导致qk点积的方差越大，导致向量元素值之间的差距变大"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);\"}"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"）。"}]}]},{"ID":"20251012164032-zn9s9dd","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012164032-zn9s9dd","updated":"20251012164032"},"Children":[{"ID":"20251012164032-vnnii74","Type":"NodeParagraph","Properties":{"id":"20251012164032-vnnii74","updated":"20251012164039"},"Children":[{"Type":"NodeText","Data":"向量元素值之间的差距变大，代入softmax后进入softmax的饱和区域，容易出现某个值接近1，其他值接近0，从而导致梯度消失。"}]}]},{"ID":"20251012164021-ffq61lf","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251012164021-ffq61lf","updated":"20251012164021"},"Children":[{"ID":"20251012164021-oq3pksp","Type":"NodeParagraph","Properties":{"id":"20251012164021-oq3pksp","updated":"20251012164104"},"Children":[{"Type":"NodeText","Data":"除以dk能缩小点积规模，让softmax输出更平缓，保证梯度正常传递，结果归一化成均值为0、方差为1的向量（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-font-background8); color: var(--b3-font-color7);"},"TextMarkType":"strong text","TextMarkTextContent":"让前后分布均值方差一致"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-font-background8); color: var(--b3-font-color7);\"}"},{"Type":"NodeText","Data":"），避免梯度消失。"}]}]}]}]},{"ID":"20251012164142-fofbxps","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012164142-fofbxps","updated":"20251012164215"},"Children":[{"ID":"20251012164142-6jbc6nb","Type":"NodeParagraph","Properties":{"id":"20251012164142-6jbc6nb","updated":"20251012164154"},"Children":[{"Type":"NodeText","Data":"为什么是根号："}]},{"ID":"20251012164212-ch07ohs","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012164212-ch07ohs","updated":"20251012164215"},"Children":[{"ID":"20251012164212-41sn4pl","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012164212-41sn4pl","updated":"20251012164212"},"Children":[{"ID":"20251012164212-2w352u9","Type":"NodeParagraph","Properties":{"id":"20251012164212-2w352u9","updated":"20251012164212"},"Children":[{"Type":"NodeText","Data":"方差要开平方，所以是根号。"}]}]},{"ID":"20251012164214-3kk2rpu","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012164214-3kk2rpu","updated":"20251012164215"},"Children":[{"ID":"20251012164214-ekzjlnn","Type":"NodeParagraph","Properties":{"id":"20251012164214-ekzjlnn","updated":"20251012164215"},"Children":[{"Type":"NodeText","Data":"q和k向量里的每一个变量是0 1分布，那么根据变量乘积的方差公式var(qk)＝var(q)var(k)+var(q)E(k)"},{"Type":"NodeTextMark","TextMarkType":"sup","TextMarkTextContent":"2+var(k)E(q)"},{"Type":"NodeText","Data":"2，E(q)和E(k)都是0，那么就是var(qk)＝var(q)var(k)，也就是qk每个变量乘积之后方差是1，qk点积的方差就是d"}]}]}]}]}]},{"ID":"20251012161559-0gkmch8","Type":"NodeParagraph","Properties":{"id":"20251012161559-0gkmch8","updated":"20251012161559"}},{"ID":"20251012162459-51hk64o","Type":"NodeParagraph","Properties":{"id":"20251012162459-51hk64o","updated":"20251012162505"},"Children":[{"Type":"NodeText","Data":"能不能换成别的数？"}]},{"ID":"20251012162505-4t76glv","Type":"NodeParagraph","Properties":{"id":"20251012162505-4t76glv","updated":"20251012162505"}},{"ID":"20251012162459-d2syp3v","Type":"NodeParagraph","Properties":{"id":"20251012162459-d2syp3v","updated":"20251012162459"}},{"ID":"20251012161559-sq60ktt","Type":"NodeParagraph","Properties":{"id":"20251012161559-sq60ktt","updated":"20251012162455"},"Children":[{"Type":"NodeText","Data":"Attention中Q和K权重相等可以吗？"}]},{"ID":"20251009171816-5cabewt","Type":"NodeParagraph","Properties":{"id":"20251009171816-5cabewt","updated":"20251009171816"}},{"ID":"20251012162711-m1fjyva","Type":"NodeParagraph","Properties":{"id":"20251012162711-m1fjyva","updated":"20251012162711"}},{"ID":"20251012162711-qdpdez2","Type":"NodeParagraph","Properties":{"id":"20251012162711-qdpdez2","updated":"20251012162712"},"Children":[{"Type":"NodeText","Data":"什么因素会导致模型训练不稳定，什么激活函数会导致训练不稳定，什么原因会导致梯度消失/爆炸"}]},{"ID":"20251012163359-youj204","Type":"NodeParagraph","Properties":{"id":"20251012163359-youj204","updated":"20251012163359"}},{"ID":"20251012163400-xldb6l5","Type":"NodeParagraph","Properties":{"id":"20251012163400-xldb6l5","updated":"20251012163400"}},{"ID":"20251012163526-pw5ab6n","Type":"NodeParagraph","Properties":{"id":"20251012163526-pw5ab6n","updated":"20251012163529"},"Children":[{"Type":"NodeText","Data":"1.Transformer为何使用多头注意力机制？（为什么不使用一个头）\n2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？ （注意和第一个问题的区别）\n3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n5.在计算attention score的时候如何对padding做mask操作？\n6.为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n7.大概讲一下Transformer的Encoder模块？\n8.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n9.简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n10.你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n11.简单讲一下Transformer中的残差结构以及意义。\n12.为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n13.简答讲一下BatchNorm技术，以及它的优缺点。\n14.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\n15.Encoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\n16.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\n17.Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n19.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n20解码端的残差结构有没有把后续未被看见的mask信息添加进来，造成信息的泄露。"}]},{"ID":"20251012163400-3f1dued","Type":"NodeParagraph","Properties":{"id":"20251012163400-3f1dued","updated":"20251012163400"}},{"ID":"20251012163400-5894i0x","Type":"NodeParagraph","Properties":{"id":"20251012163400-5894i0x","updated":"20251012163400"}}]}