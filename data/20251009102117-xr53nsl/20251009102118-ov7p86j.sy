{"ID":"20251009102118-ov7p86j","Spec":"1","Type":"NodeDocument","Properties":{"custom-picgo-file-map-key":"\u0026#123;\u0026quot;3836c346af68ed4172472e5dbaf2a6ac\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251012165721-fsackoo.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;3836c346af68ed4172472e5dbaf2a6ac\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251012165721-fsackoo.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012165748.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;20251012162135-qza7x2g\u0026quot;\u0026#125;,\u0026quot;8ec3c219d386d40c793fd3e90fc5acd4\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251012170525-opr7jp8.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;8ec3c219d386d40c793fd3e90fc5acd4\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251012170525-opr7jp8.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012170526.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;\u0026quot;\u0026#125;,\u0026quot;39d66507c3b50859c83b4257721a223b\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251108204707-iis4yvb.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;39d66507c3b50859c83b4257721a223b\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251108204707-iis4yvb.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251108204707.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;\u0026quot;\u0026#125;,\u0026quot;907274a35e7cbc3cd65592550d10a7fb\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251108204907-jwwd35s.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;907274a35e7cbc3cd65592550d10a7fb\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251108204907-jwwd35s.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251108204907.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;\u0026quot;\u0026#125;,\u0026quot;d46eadc2baba449651a5b46593dcd1cd\u0026quot;:\u0026#123;\u0026quot;name\u0026quot;:\u0026quot;image-20251108205134-ehova3i.png\u0026quot;,\u0026quot;hash\u0026quot;:\u0026quot;d46eadc2baba449651a5b46593dcd1cd\u0026quot;,\u0026quot;originUrl\u0026quot;:\u0026quot;assets/image-20251108205134-ehova3i.png\u0026quot;,\u0026quot;url\u0026quot;:\u0026quot;https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251108205135.png\u0026quot;,\u0026quot;alt\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;title\u0026quot;:\u0026quot;image.png\u0026quot;,\u0026quot;isLocal\u0026quot;:false,\u0026quot;blockId\u0026quot;:\u0026quot;\u0026quot;\u0026#125;\u0026#125;","icon":"1f427","id":"20251009102118-ov7p86j","title":"八股","type":"doc","updated":"20251108205321"},"Children":[{"ID":"20251009102119-bk79nfo","Type":"NodeHeading","HeadingLevel":1,"Properties":{"id":"20251009102119-bk79nfo","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"大模型基础"}]},{"ID":"20251009153935-3my4s8g","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20251009153935-3my4s8g","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"机制部分"}]},{"ID":"20251009153946-a1q9qdf","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251009153946-a1q9qdf","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"注意力"}]},{"ID":"20251012184734-qybbq7j","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20251012184734-qybbq7j","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"基础注意力"}]},{"ID":"20251009153412-yykcka0","Type":"NodeMathBlock","Properties":{"id":"20251009153412-yykcka0","updated":"20251009153923"},"Children":[{"Type":"NodeMathBlockOpenMarker"},{"Type":"NodeMathBlockContent","Data":"Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V"},{"Type":"NodeMathBlockCloseMarker"}]},{"ID":"20251012230606-6yabwpj","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012230606-6yabwpj","updated":"20251012230607"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251009153923-8je37xh","Type":"NodeParagraph","Properties":{"id":"20251009153923-8je37xh","updated":"20251012230607"},"Children":[{"Type":"NodeText","Data":"注意力的过程?"}]},{"ID":"20251009154203-tuvpfcq","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251009154203-tuvpfcq","updated":"20251012230607"},"Children":[{"ID":"20251009154205-tkdo68p","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251009154205-tkdo68p","updated":"20251009154556"},"Children":[{"ID":"20251009154205-fglxmb0","Type":"NodeParagraph","Properties":{"id":"20251009154205-fglxmb0","updated":"20251009154556"},"Children":[{"Type":"NodeText","Data":"对于输入序列的每个位置，通过"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"点积"},{"Type":"NodeText","Data":"计算其与其他位置之间的相似度得分"}]}]},{"ID":"20251009154236-1a0l4zk","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251009154236-1a0l4zk","updated":"20251012230528"},"Children":[{"ID":"20251009154236-cvyxp3j","Type":"NodeParagraph","Properties":{"id":"20251009154236-cvyxp3j","updated":"20251012230528"},"Children":[{"Type":"NodeText","Data":"然后对得分进行放缩处理，以防止"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"梯度消失"}]}]},{"ID":"20251009171345-m7fwpby","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251009171345-m7fwpby","updated":"20251009171426"},"Children":[{"ID":"20251009171345-80siu0h","Type":"NodeParagraph","Properties":{"id":"20251009171345-80siu0h","updated":"20251009171426"},"Children":[{"Type":"NodeText","Data":"将得分用"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"softmax"},{"Type":"NodeText","Data":"​函数转换为注意力权重，以便计算每个位置的加权和"}]}]},{"ID":"20251012230421-yzfdo8i","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"NC4=","Num":4},"Properties":{"id":"20251012230421-yzfdo8i","updated":"20251012230425"},"Children":[{"ID":"20251012230421-zk5iaf1","Type":"NodeParagraph","Properties":{"id":"20251012230421-zk5iaf1","updated":"20251012230425"},"Children":[{"Type":"NodeText","Data":"使用注意力权重对输入序列中的所有位置进行加权求和，得到每个位置的自注意输出。"}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251009154247-r0cts7t","Type":"NodeParagraph","Properties":{"id":"20251009154247-r0cts7t","updated":"20251009154247"}},{"ID":"20251012230551-0o9f2vy","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012230551-0o9f2vy","updated":"20251012231145"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251009154247-77nyjqd","Type":"NodeParagraph","Properties":{"id":"20251009154247-77nyjqd","updated":"20251012230552"},"Children":[{"Type":"NodeText","Data":"为什么要除以根号dk？能换成别的数吗？"}]},{"ID":"20251012162135-qza7x2g","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012162135-qza7x2g","updated":"20251012231145"},"Children":[{"ID":"20251012165751-2nkunul","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-2nkunul","updated":"20251012231145"},"Children":[{"ID":"20251012165751-zsw88nk","Type":"NodeParagraph","Properties":{"id":"20251012165751-zsw88nk","updated":"20251012231112"},"Children":[{"Type":"NodeText","Data":"避免softmax"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"梯度消失（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-card-success-background); color: var(--b3-card-success-color);"},"TextMarkType":"strong text mark","TextMarkTextContent":"注意不是爆炸"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-card-success-background); color: var(--b3-card-success-color);\"}"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"）"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"："}]},{"ID":"20251012165751-wnnnzy2","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012165751-wnnnzy2","updated":"20251012231145"},"Children":[{"ID":"20251012165751-ub9re1o","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-ub9re1o","updated":"20251012231137"},"Children":[{"ID":"20251012165751-cxkqkeg","Type":"NodeParagraph","Properties":{"id":"20251012165751-cxkqkeg","updated":"20251012231137"},"Children":[{"Type":"NodeText","Data":"当dk（Query/Key的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"维度"},{"Type":"NodeText","Data":"）"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"较大"},{"Type":"NodeText","Data":"时，"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"Q和K的点积结果会随"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"dk增大而显著变大"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);"},"TextMarkType":"em","TextMarkTextContent":"根号dk越大，维度变多，累加变多，导致qk点积的方差越大，导致向量元素值之间的差距变大"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);\"}"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"）。"}]}]},{"ID":"20251012165751-n3a6o1w","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012165751-n3a6o1w","updated":"20251012165751"},"Children":[{"ID":"20251012165751-9koqiuw","Type":"NodeParagraph","Properties":{"id":"20251012165751-9koqiuw","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"向量元素值之间的差距变大，代入softmax后进入softmax的饱和区域，容易出现某个值接近1，其他值接近0，从而导致梯度消失。"}]}]},{"ID":"20251012165751-nsny473","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251012165751-nsny473","updated":"20251012231145"},"Children":[{"ID":"20251012165751-7k38r6a","Type":"NodeParagraph","Properties":{"id":"20251012165751-7k38r6a","updated":"20251012231145"},"Children":[{"Type":"NodeText","Data":"除以dk能缩小点积规模，让softmax输出更平缓，保证梯度正常传递，结果归一化成均值为0、方差为1的向量（"},{"Type":"NodeTextMark","Properties":{"style":"background-color: var(--b3-font-background8); color: var(--b3-font-color7);"},"TextMarkType":"strong mark","TextMarkTextContent":"让前后分布均值方差一致"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"background-color: var(--b3-font-background8); color: var(--b3-font-color7);\"}"},{"Type":"NodeText","Data":"），避免梯度消失。"}]}]}]}]},{"ID":"20251012165751-cpbj60e","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012165751-cpbj60e","updated":"20251012170530"},"Children":[{"ID":"20251012165751-ecu0nwh","Type":"NodeParagraph","Properties":{"id":"20251012165751-ecu0nwh","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"为什么是根号："}]},{"ID":"20251012165751-0yl9lx3","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012165751-0yl9lx3","updated":"20251012170530"},"Children":[{"ID":"20251012165751-0eze3lc","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-0eze3lc","updated":"20251012165751"},"Children":[{"ID":"20251012165751-bng9hvc","Type":"NodeParagraph","Properties":{"id":"20251012165751-bng9hvc","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"q和k向量里的每一个变量是0 1分布，那么根据变量乘积的方差公式 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":" var(qk)＝var(q)var(k)+var(q)E(k)^2+var(k)E(q)^2"},{"Type":"NodeText","Data":" ，E(q)和E(k)都是0，那么就是 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"var(qk)＝var(q)var(k)"},{"Type":"NodeText","Data":"，也就是qk每个变量乘积之后方差是1，qk点积的"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"方差"},{"Type":"NodeText","Data":"就是dk"}]}]},{"ID":"20251012165751-7c2au37","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"Mi4=","Num":2},"Properties":{"id":"20251012165751-7c2au37","updated":"20251012170530"},"Children":[{"ID":"20251012165751-hdqpg62","Type":"NodeParagraph","Properties":{"id":"20251012165751-hdqpg62","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"方差是dk，方差是平方，所以除以根号dk"}]},{"ID":"20251012165751-im2htrn","Type":"NodeParagraph","Properties":{"id":"20251012165751-im2htrn","updated":"20251012170413"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Properties":{"style":"width: 621px;"},"Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012165748.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"width: 621px;\"}"},{"Type":"NodeText","Data":"​"}]},{"ID":"20251012170524-izhc5w0","Type":"NodeParagraph","Properties":{"id":"20251012170524-izhc5w0","updated":"20251012170530"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Properties":{"style":"width: 603px;"},"Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251012170526.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"width: 603px;\"}"},{"Type":"NodeText","Data":"​"}]}]}]}]},{"ID":"20251012165751-hhyp6jg","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"My4=","Num":3},"Properties":{"id":"20251012165751-hhyp6jg","updated":"20251012170736"},"Children":[{"ID":"20251012165751-om1x2kl","Type":"NodeParagraph","Properties":{"id":"20251012165751-om1x2kl","updated":"20251012165751"},"Children":[{"Type":"NodeText","Data":"能换成别的数吗？"}]},{"ID":"20251012165751-7009y4j","Type":"NodeList","ListData":{"Typ":1},"Properties":{"id":"20251012165751-7009y4j","updated":"20251012170736"},"Children":[{"ID":"20251012165751-dzygz4n","Type":"NodeListItem","ListData":{"Typ":1,"Delimiter":46,"Marker":"MS4=","Num":1},"Properties":{"id":"20251012165751-dzygz4n","updated":"20251012170736"},"Children":[{"ID":"20251012165751-8x8u94k","Type":"NodeParagraph","Properties":{"id":"20251012165751-8x8u94k","updated":"20251012170736"},"Children":[{"Type":"NodeText","Data":"根号dk能够让数据方差分布更好"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012161559-0gkmch8","Type":"NodeParagraph","Properties":{"id":"20251012161559-0gkmch8","updated":"20251012161559"}},{"ID":"20251012162459-d2syp3v","Type":"NodeParagraph","Properties":{"id":"20251012162459-d2syp3v","updated":"20251012162459"}},{"ID":"20251012231203-0w5fa93","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231203-0w5fa93","updated":"20251012231237"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012161559-sq60ktt","Type":"NodeParagraph","Properties":{"id":"20251012161559-sq60ktt","updated":"20251012170822"},"Children":[{"Type":"NodeText","Data":"Attention的时间复杂度是多少？"}]},{"ID":"20251012171821-1wwfn0i","Type":"NodeBlockquote","Properties":{"id":"20251012171821-1wwfn0i","updated":"20251012231228"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012171822-9a1sg0f","Type":"NodeParagraph","Properties":{"id":"20251012171822-9a1sg0f","updated":"20251012231228"},"Children":[{"Type":"NodeText","Data":"根据矩阵乘法规则：m×k 矩阵与 k×p 矩阵相乘，时间复杂度是"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"m（行）×k（矩阵相交）×p（列）"}]}]},{"ID":"20251012170822-h61bs4a","Type":"NodeList","ListData":{},"Properties":{"id":"20251012170822-h61bs4a","updated":"20251012231237"},"Children":[{"ID":"20251012170852-8yrouco","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012170852-8yrouco","updated":"20251012231237"},"Children":[{"ID":"20251012170852-tl4zxwm","Type":"NodeParagraph","Properties":{"id":"20251012170852-tl4zxwm","updated":"20251012231237"},"Children":[{"Type":"NodeText","Data":"输入序列长度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"，特征维度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"d"},{"Type":"NodeText","Data":"。总时间复杂度为"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"mark"},{"Type":"NodeText","Data":" "},{"Type":"NodeTextMark","TextMarkType":"inline-math mark","TextMarkInlineMathContent":"O(nd^2)(\\text{线性变换})+O(n^2d)(\\text{点积与加权和})+O(n^2)(\\mathrm{softmax})"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012170920-pcv2jhk","Type":"NodeList","ListData":{},"Properties":{"id":"20251012170920-pcv2jhk","updated":"20251012183020"},"Children":[{"ID":"20251012170919-l3knork","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012170919-l3knork","updated":"20251012170919"},"Children":[{"ID":"20251012170919-993idfu","Type":"NodeParagraph","Properties":{"id":"20251012170919-993idfu","updated":"20251012171247"},"Children":[{"Type":"NodeText","Data":"第一步 QKV的线性变换：计算 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"XW^Q、XW^V、XW^K"},{"Type":"NodeText","Data":"，每个token计算一次，计算n次，矩阵计算两个for循环，长度为d，所以为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n·d^2)"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012171518-vidy11o","Type":"NodeList","ListData":{},"Properties":{"id":"20251012171518-vidy11o","updated":"20251012171518"},"Children":[{"ID":"20251012171518-i9mty7h","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171518-i9mty7h","updated":"20251012171518"},"Children":[{"ID":"20251012171518-zo0ovnj","Type":"NodeParagraph","Properties":{"id":"20251012171518-zo0ovnj","updated":"20251012171840"},"Children":[{"Type":"NodeText","Data":"Q 的形状是 n×d（n 个 token，每个 d 维）"}]}]}]}]},{"ID":"20251012171248-isvr0wq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171248-isvr0wq","updated":"20251012171248"},"Children":[{"ID":"20251012171248-k7mqk6g","Type":"NodeParagraph","Properties":{"id":"20251012171248-k7mqk6g","updated":"20251012171724"},"Children":[{"Type":"NodeText","Data":"第二步 计算注意力分数：计算"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"QK^T_esc_newline_"},{"Type":"NodeText","Data":"，n×d与d×n的矩阵相乘，时间复杂度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n^2·d)"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012171901-ryjktg5","Type":"NodeList","ListData":{},"Properties":{"id":"20251012171901-ryjktg5","updated":"20251012171901"},"Children":[{"ID":"20251012171901-e9vn30u","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171901-e9vn30u","updated":"20251012171901"},"Children":[{"ID":"20251012171901-3swja8i","Type":"NodeParagraph","Properties":{"id":"20251012171901-3swja8i","updated":"20251012171924"},"Children":[{"Type":"NodeText","Data":"这一步是瓶颈，因为n平方，随着序列长度增长会变大"}]}]},{"ID":"20251012172028-et75x7k","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172028-et75x7k","updated":"20251012172028"},"Children":[{"ID":"20251012172028-yq64bni","Type":"NodeParagraph","Properties":{"id":"20251012172028-yq64bni","updated":"20251012172035"},"Children":[{"Type":"NodeText","Data":"输出结果为n×n"}]}]}]}]},{"ID":"20251012171924-7j7jgzs","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012171924-7j7jgzs","updated":"20251012172101"},"Children":[{"ID":"20251012171924-94zb6dc","Type":"NodeParagraph","Properties":{"id":"20251012171924-94zb6dc","updated":"20251012172001"},"Children":[{"Type":"NodeText","Data":"第三步 softmax： "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n^2)"},{"Type":"NodeText","Data":"\n"}]},{"ID":"20251012172101-2mq9626","Type":"NodeList","ListData":{},"Properties":{"id":"20251012172101-2mq9626","updated":"20251012172101"},"Children":[{"ID":"20251012172101-hdsymql","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172101-hdsymql","updated":"20251012172101"},"Children":[{"ID":"20251012172101-dvk7gik","Type":"NodeParagraph","Properties":{"id":"20251012172101-dvk7gik","updated":"20251012172101"},"Children":[{"Type":"NodeText","Data":"第一步：对该行每个元素求指数（exp），共 n 次操作；"}]}]},{"ID":"20251012172101-5wjipka","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172101-5wjipka","updated":"20251012172101"},"Children":[{"ID":"20251012172101-3im0ihy","Type":"NodeParagraph","Properties":{"id":"20251012172101-3im0ihy","updated":"20251012172101"},"Children":[{"Type":"NodeText","Data":"第二步：计算该行所有元素的 exp 和（分母），再用每个元素的 exp 除以这个和，又需 n 次操作。\n每行总操作数为 n + n "},{"Type":"NodeBackslash","Data":"span","Children":[{"Type":"NodeText","Data":"="}]},{"Type":"NodeText","Data":" 2n，n 行则总操作数为 n×2n "},{"Type":"NodeBackslash","Data":"span","Children":[{"Type":"NodeText","Data":"="}]},{"Type":"NodeText","Data":" O (n²)（常数 2 可忽略，复杂度取主导项）。"}]}]}]}]},{"ID":"20251012172124-i9bla01","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012172124-i9bla01","updated":"20251012183020"},"Children":[{"ID":"20251012172124-63aozch","Type":"NodeParagraph","Properties":{"id":"20251012172124-63aozch","updated":"20251012172129"},"Children":[{"Type":"NodeText","Data":"添加上V："}]},{"ID":"20251012182903-dh1qqk7","Type":"NodeList","ListData":{},"Properties":{"id":"20251012182903-dh1qqk7","updated":"20251012183020"},"Children":[{"ID":"20251012182902-ub7x1or","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012182902-ub7x1or","updated":"20251012183020"},"Children":[{"ID":"20251012182902-0gfe25e","Type":"NodeParagraph","Properties":{"id":"20251012182902-0gfe25e","updated":"20251012183020"},"Children":[{"Type":"NodeText","Data":"计算"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"softmax(Q·KT)·V"},{"Type":"NodeText","Data":"的复杂度是"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"O(n^2·d)"},{"Type":"NodeText","Data":"。因为每个注意力权重需要乘以对应的Value向量。"}]}]}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251009171816-5cabewt","Type":"NodeParagraph","Properties":{"id":"20251009171816-5cabewt","updated":"20251009171816"}},{"ID":"20251012162711-m1fjyva","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20251012162711-m1fjyva","updated":"20251019172705"},"Children":[{"Type":"NodeText","Data":"多头注意力"}]},{"ID":"20251012231305-qmyy076","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231305-qmyy076","updated":"20251012231346"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012183148-l14zjsp","Type":"NodeParagraph","Properties":{"id":"20251012183148-l14zjsp","updated":"20251012231305"},"Children":[{"Type":"NodeText","Data":"Transformer为何使用多头注意力机制？（为什么不使用一个头）"}]},{"ID":"20251012183309-f2ib6xh","Type":"NodeList","ListData":{},"Properties":{"id":"20251012183309-f2ib6xh","updated":"20251012231346"},"Children":[{"ID":"20251012184053-mvx0jve","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184053-mvx0jve","updated":"20251012231320"},"Children":[{"ID":"20251012184053-770zit6","Type":"NodeParagraph","Properties":{"id":"20251012184053-770zit6","updated":"20251012231320"},"Children":[{"Type":"NodeText","Data":"核心思想：将Q,K,V通过"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"不同的线性变换"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"（投影）映射到"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"多个不同的子空间"},{"Type":"NodeText","Data":"(subspace)中，在每个子空间里"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"独立地"},{"Type":"NodeText","Data":"执行缩放点积注意力(Scaled Dot-Product Attention),最后将所有"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"子空间的注意力输出拼接起来"},{"Type":"NodeText","Data":"，再进行一次线性变换得到最终的输出。"}]}]},{"ID":"20251012184106-k7dw8gh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184106-k7dw8gh","updated":"20251012231346"},"Children":[{"ID":"20251012184106-wpk0izl","Type":"NodeParagraph","Properties":{"id":"20251012184106-wpk0izl","updated":"20251012184106"},"Children":[{"Type":"NodeText","Data":"作用与好处："}]},{"ID":"20251012184109-qonmnlk","Type":"NodeList","ListData":{},"Properties":{"id":"20251012184109-qonmnlk","updated":"20251012231346"},"Children":[{"ID":"20251012184109-ig40dpl","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184109-ig40dpl","updated":"20251012231338"},"Children":[{"ID":"20251012184109-0nji5ko","Type":"NodeParagraph","Properties":{"id":"20251012184109-0nji5ko","updated":"20251012231329"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"允许模型关注来自不同子空间的不同信息："},{"Type":"NodeText","Data":"这是最关键的作用。"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text","TextMarkTextContent":"单头注意力机制可能会让模型只关注"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text mark","TextMarkTextContent":"某一种特定类型"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text","TextMarkTextContent":"的关联信息（比如语法关系），或者所有"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeTextMark","Properties":{"style":"color: var(--b3-font-color4);"},"TextMarkType":"strong u text mark","TextMarkTextContent":"信息被平均化"},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"color: var(--b3-font-color4);\"}"},{"Type":"NodeText","Data":"。多头机制允许不同的\"头”\"(head)学习关注输入序列中不同方面的信息。"}]},{"ID":"20251012184457-adyy54b","Type":"NodeList","ListData":{},"Properties":{"id":"20251012184457-adyy54b","updated":"20251012231338"},"Children":[{"ID":"20251012184207-pxrx6ae","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184207-pxrx6ae","updated":"20251012184510"},"Children":[{"ID":"20251012184207-8aeozsg","Type":"NodeBlockquote","Properties":{"id":"20251012184207-8aeozsg","updated":"20251012184207"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012184207-cwpbybp","Type":"NodeParagraph","Properties":{"id":"20251012184207-cwpbybp","updated":"20251012184207"},"Children":[{"Type":"NodeText","Data":"例如，一个头可能关注局部的语法依赖，另一个头可能关注长距离的语义关联，还有一个头可能关注词语的位置信息等。这使得模型能够更全面、更细致地捕捉输入信息中丰富的特征和依赖关系。"}]}]}]},{"ID":"20251012184607-6cuz29n","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184607-6cuz29n","updated":"20251012231334"},"Children":[{"ID":"20251012184607-ryg70rv","Type":"NodeParagraph","Properties":{"id":"20251012184607-ryg70rv","updated":"20251012231334"},"Children":[{"Type":"NodeText","Data":"提供更丰富的表示能力：每个头可以看作是在"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"不同的表示视角"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"下进行注意力计算"},{"Type":"NodeText","Data":"。将这些不同视角的结果结合起来，可以产生比单视角更强大、更鲁棒的特征表示。"}]}]},{"ID":"20251012184636-vnydbb8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184636-vnydbb8","updated":"20251012231338"},"Children":[{"ID":"20251012184636-1mk670o","Type":"NodeParagraph","Properties":{"id":"20251012184636-1mk670o","updated":"20251012231338"},"Children":[{"Type":"NodeText","Data":"类似集成学习的效果：在某种程度上，多头注意力有点像"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"集成学习(Ensemble Learning)"},{"Type":"NodeText","Data":"的思想，不同的头可以看作是不同的“专家”，各自专注于序列的不同方面，最后综合它们的意见，得到更优的整体判断"}]}]}]}]},{"ID":"20251012184640-3x5yzlw","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012184640-3x5yzlw","updated":"20251012231346"},"Children":[{"ID":"20251012184640-sf3chef","Type":"NodeParagraph","Properties":{"id":"20251012184640-sf3chef","updated":"20251012231346"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"稳定训练过程（间接作用）"},{"Type":"NodeText","Data":"：将"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"原始的高维空间分解为多个低维子空间"},{"Type":"NodeText","Data":"进行计算，可能有助于稳定训练，降低模型对某些特定模式过度敏感的风险。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012184655-4ci9w2m","Type":"NodeParagraph","Properties":{"id":"20251012184655-4ci9w2m","updated":"20251012184655"}},{"ID":"20251012184655-aej1wih","Type":"NodeParagraph","Properties":{"id":"20251012184655-aej1wih","updated":"20251012184655"}},{"ID":"20251012231359-6dlt12t","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231359-6dlt12t","updated":"20251012231410"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012183309-0m2kl6c","Type":"NodeParagraph","Properties":{"id":"20251012183309-0m2kl6c","updated":"20251012231400"},"Children":[{"Type":"NodeText","Data":"多头注意力的时空复杂度？"}]},{"ID":"20251012223353-nu9nzrt","Type":"NodeBlockquote","Properties":{"id":"20251012223353-nu9nzrt","updated":"20251012231400"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012223354-2fskso4","Type":"NodeParagraph","Properties":{"id":"20251012223354-2fskso4","updated":"20251012223437"},"Children":[{"Type":"NodeText","Data":"相似问题："},{"Type":"NodeTextMark","TextMarkType":"block-ref","TextMarkBlockRefID":"20251012161559-sq60ktt","TextMarkBlockRefSubtype":"d","TextMarkTextContent":"Attention的时间复杂度是多少？"}]}]},{"ID":"20251012223304-btral2z","Type":"NodeParagraph","Properties":{"id":"20251012223304-btral2z","updated":"20251012231400"},"Children":[{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"n"},{"Type":"NodeText","Data":"个输入向量的序列，每个向量的维度为"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"d"}]},{"ID":"20251012183319-ipu614d","Type":"NodeList","ListData":{},"Properties":{"id":"20251012183319-ipu614d","updated":"20251012231410"},"Children":[{"ID":"20251012223243-inhesy9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012223243-inhesy9","updated":"20251012231410"},"Children":[{"ID":"20251012223243-t5j2h2s","Type":"NodeParagraph","Properties":{"id":"20251012223243-t5j2h2s","updated":"20251012223246"},"Children":[{"Type":"NodeText","Data":"时间复杂度："}]},{"ID":"20251012223247-g2xdzt6","Type":"NodeList","ListData":{},"Properties":{"id":"20251012223247-g2xdzt6","updated":"20251012231410"},"Children":[{"ID":"20251012223351-t2m1cxz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012223351-t2m1cxz","updated":"20251012231410"},"Children":[{"ID":"20251012223351-lzlzlm2","Type":"NodeParagraph","Properties":{"id":"20251012223351-lzlzlm2","updated":"20251012231410"},"Children":[{"Type":"NodeText","Data":"多头计算：如果使用个头的多头注意力机制，那么需要对每个头分别进行上述计算，因此总的时间复杂度"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"为"},{"Type":"NodeTextMark","TextMarkType":"inline-math mark","TextMarkInlineMathContent":"O(hn^2d)"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"。"}]}]}]}]}]},{"ID":"20251012183126-skz0338","Type":"NodeParagraph","Properties":{"id":"20251012183126-skz0338","style":"background-color: var(--b3-font-background1); --b3-parent-background: var(--b3-font-background1);","updated":"20251012231400"},"Children":[{"Type":"NodeText","Data":"虽然每个头都独立地计算注意力权重，但是这些计算可以在现代硬件（如GPU）上并行执行，提高了计算效率。"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012224145-gjp5tpv","Type":"NodeParagraph","Properties":{"id":"20251012224145-gjp5tpv","updated":"20251012224145"}},{"ID":"20251019172703-6jnnwsl","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251019172703-6jnnwsl","updated":"20251108205321"},"Children":[{"Type":"NodeText","Data":"FNN部分"}]},{"ID":"20251019173242-sz3793m","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251019173242-sz3793m","updated":"20251019172807"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251019172709-i2glhvq","Type":"NodeParagraph","Properties":{"id":"20251019172709-i2glhvq","updated":"20251019173243"},"Children":[{"Type":"NodeText","Data":"为什么transformer的FFN需要先升维再降维"}]},{"ID":"20251019172728-i4cc9xl","Type":"NodeParagraph","Properties":{"id":"20251019172728-i4cc9xl","updated":"20251019173243"},"Children":[{"Type":"NodeText","Data":"FFN通常把输入从512维扩展到2048维，再投影回512维。"}]},{"ID":"20251019172748-mcz5ryi","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172748-mcz5ryi","updated":"20251019172807"},"Children":[{"ID":"20251019172748-3j3a9fv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172748-3j3a9fv","updated":"20251019172807"},"Children":[{"ID":"20251019172748-ofonlgb","Type":"NodeParagraph","Properties":{"id":"20251019172748-ofonlgb","updated":"20251108205045"},"Children":[{"Type":"NodeText","Data":"同维度表达有上限，升维后可抽取高阶特征："}]},{"ID":"20251019172807-vkmflkj","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172807-vkmflkj","updated":"20251019172807"},"Children":[{"ID":"20251019172807-ntzu5vj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172807-ntzu5vj","updated":"20251019172807"},"Children":[{"ID":"20251019172807-m4wqms9","Type":"NodeParagraph","Properties":{"id":"20251019172807-m4wqms9","updated":"20251019172823"},"Children":[{"Type":"NodeText","Data":"直接用512→512的线性层，本质上就是矩阵乘法，无论叠多少层都等价于"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"一个线性变换"},{"Type":"NodeText","Data":"。"}]}]},{"ID":"20251019172811-7k5ndtd","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172811-7k5ndtd","updated":"20251019172811"},"Children":[{"ID":"20251019172811-bvh5luf","Type":"NodeParagraph","Properties":{"id":"20251019172811-bvh5luf","updated":"20251019172828"},"Children":[{"Type":"NodeText","Data":"加了激活函数后确实引入了非线性，但在同维度空间里，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"特征的变换能力"},{"Type":"NodeText","Data":"是有上限的。"}]}]},{"ID":"20251019172829-0ew7kim","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172829-0ew7kim","updated":"20251019172829"},"Children":[{"ID":"20251019172829-ifndvit","Type":"NodeParagraph","Properties":{"id":"20251019172829-ifndvit","updated":"20251019172841"},"Children":[{"Type":"NodeText","Data":"升到2048维后，激活函数能在"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"更高维度的空间里切分数据"},{"Type":"NodeText","Data":"，创造出原本512维空间里不存在的"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"特征组合"},{"Type":"NodeText","Data":"。降维过程则是把高维空间学到的复杂特征"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"映射回原始维度"},{"Type":"NodeText","Data":"，相当于做了一次信息提炼。"}]},{"ID":"20251108205100-g2ldw1s","Type":"NodeList","ListData":{},"Properties":{"id":"20251108205100-g2ldw1s","updated":"20251108205100"},"Children":[{"ID":"20251108205047-gj2jafy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108205047-gj2jafy","updated":"20251108205047"},"Children":[{"ID":"20251108205047-nu7evwd","Type":"NodeBlockquote","Properties":{"id":"20251108205047-nu7evwd","updated":"20251108205102"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251108205102-yqj9mqv","Type":"NodeParagraph","Properties":{"id":"20251108205102-yqj9mqv","updated":"20251108205122"},"Children":[{"Type":"NodeText","Data":"举例："},{"Type":"NodeTextMark","TextMarkType":"em","TextMarkTextContent":"二维坐标如果要找一条直线对点分类是比较困难的，所以扩充一个维度投影到三维空间，可用平面分类了"},{"Type":"NodeText","Data":"。"}]}]}]},{"ID":"20251108205124-1hht0sp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108205124-1hht0sp","updated":"20251108205124"},"Children":[{"ID":"20251108205124-azkix49","Type":"NodeParagraph","Properties":{"id":"20251108205124-azkix49","updated":"20251108205139"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251108205135.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeText","Data":"​"}]}]}]}]},{"ID":"20251108205144-9zgppuz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108205144-9zgppuz","updated":"20251108205144"},"Children":[{"ID":"20251108205144-vzkejiz","Type":"NodeParagraph","Properties":{"id":"20251108205144-vzkejiz","updated":"20251108205144"}}]}]}]},{"ID":"20251019172844-b8tnq1f","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172844-b8tnq1f","updated":"20251019172844"},"Children":[{"ID":"20251019172844-sxm90pw","Type":"NodeParagraph","Properties":{"id":"20251019172844-sxm90pw","updated":"20251019172851"},"Children":[{"Type":"NodeText","Data":"高维切分更复杂："}]},{"ID":"20251019172852-2jarmy6","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172852-2jarmy6","updated":"20251019172852"},"Children":[{"ID":"20251019172851-0ytylvm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172851-0ytylvm","updated":"20251019172851"},"Children":[{"ID":"20251019172851-hx7v6ye","Type":"NodeParagraph","Properties":{"id":"20251019172851-hx7v6ye","updated":"20251019172922"},"Children":[{"Type":"NodeText","Data":"ReLU的本质是分段线性函数，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"每个神经元对应一个超平面"},{"Type":"NodeText","Data":"。512维空间只能用512个超平面切分，能表示的"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"函数复杂度"},{"Type":"NodeText","Data":"有限。"}]}]},{"ID":"20251019172909-yab91w0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172909-yab91w0","updated":"20251019172909"},"Children":[{"ID":"20251019172909-7ezxykh","Type":"NodeParagraph","Properties":{"id":"20251019172909-7ezxykh","updated":"20251019172909"},"Children":[{"Type":"NodeText","Data":"升到2048维后，有2048个超平面参与切分，能构建的分段线性区域数量呈指数级增长。这些高维区域对应着不同的语义模式，降维时这些复杂的高维模式被线性组合投影回512维，保留了关键信息但维度回到了原点"}]}]}]}]},{"ID":"20251019172954-qcmfrwy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019172954-qcmfrwy","updated":"20251019172954"},"Children":[{"ID":"20251019172954-kgka122","Type":"NodeParagraph","Properties":{"id":"20251019172954-kgka122","updated":"20251019173010"},"Children":[{"Type":"NodeText","Data":"参数量决定容量："}]},{"ID":"20251019173011-uxzzy4h","Type":"NodeList","ListData":{},"Properties":{"id":"20251019173011-uxzzy4h","updated":"20251019173011"},"Children":[{"ID":"20251019173011-dw4tzbo","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019173011-dw4tzbo","updated":"20251019173011"},"Children":[{"ID":"20251019173011-lh8e25e","Type":"NodeParagraph","Properties":{"id":"20251019173011-lh8e25e","updated":"20251019173039"},"Children":[{"Type":"NodeText","Data":"神经网络的参数本质上在存储"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"输入输出的映射"},{"Type":"NodeText","Data":"关系。"}]}]},{"ID":"20251019173034-tpng4gn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019173034-tpng4gn","updated":"20251019173034"},"Children":[{"ID":"20251019173034-obaylcm","Type":"NodeParagraph","Properties":{"id":"20251019173034-obaylcm","updated":"20251019173049"},"Children":[{"Type":"NodeText","Data":"512→512的FFN只有512²≈26万参数，512→2048→512有约210万参数，足足8倍的差距。这些参数可以看成key-value记忆，第一层学习\""},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"什么样的输入模式应该被识别"},{"Type":"NodeText","Data":"\"，第二层学习\""},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"识别到这个模式后应该输出什么"},{"Type":"NodeText","Data":"\"。参数量越大，能记住的模式越多"}]}]}]}]},{"ID":"20251108204759-35s5t09","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108204759-35s5t09","updated":"20251108205007"},"Children":[{"ID":"20251108204628-ua79rjt","Type":"NodeParagraph","Properties":{"id":"20251108204628-ua79rjt","updated":"20251108204811"},"Children":[{"Type":"NodeText","Data":" 【❌错误观点】激活函数会降秩"}]},{"ID":"20251108204640-x6ondg8","Type":"NodeList","ListData":{},"Properties":{"id":"20251108204640-x6ondg8","updated":"20251108205007"},"Children":[{"ID":"20251108204640-qp5ildj","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108204640-qp5ildj","updated":"20251108204640"},"Children":[{"ID":"20251108204640-obnu6q6","Type":"NodeParagraph","Properties":{"id":"20251108204640-obnu6q6","updated":"20251108204735"},"Children":[{"Type":"NodeText","Data":"​"},{"Type":"NodeImage","Data":"span","Properties":{"style":"height: 31vh;"},"Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251108204707.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"height: 31vh;\"}"},{"Type":"NodeText","Data":"​"}]}]},{"ID":"20251108204718-y43ce6x","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108204718-y43ce6x","updated":"20251108204718"},"Children":[{"ID":"20251108204718-poujth2","Type":"NodeParagraph","Properties":{"id":"20251108204718-poujth2","updated":"20251108204818"},"Children":[{"Type":"NodeText","Data":"矩阵的秩的大小"},{"Type":"NodeTextMark","TextMarkType":"strong u","TextMarkTextContent":"不能直接等同"},{"Type":"NodeText","Data":"其信息携带量，只能说明其信息携带能力！秩越大不等于信息就越多"}]},{"ID":"20251108204821-ssfv6ck","Type":"NodeList","ListData":{},"Properties":{"id":"20251108204821-ssfv6ck","updated":"20251108204821"},"Children":[{"ID":"20251108204821-3rmq6dn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108204821-3rmq6dn","updated":"20251108204821"},"Children":[{"ID":"20251108204821-qz5foh5","Type":"NodeParagraph","Properties":{"id":"20251108204821-qz5foh5","updated":"20251108204923"},"Children":[{"Type":"NodeText","Data":"例如"},{"Type":"NodeImage","Data":"span","Properties":{"style":"height: 16vh"},"Children":[{"Type":"NodeBang"},{"Type":"NodeOpenBracket"},{"Type":"NodeLinkText","Data":"image"},{"Type":"NodeCloseBracket"},{"Type":"NodeOpenParen"},{"Type":"NodeLinkDest","Data":"https://raw.githubusercontent.com/jjq0425/siyuan-img-base/main/siyuan/20251108204907.png"},{"Type":"NodeCloseParen"}]},{"Type":"NodeKramdownSpanIAL","Data":"{: style=\"height: 16vh\"}"},{"Type":"NodeText","Data":"​"}]}]}]}]},{"ID":"20251108204924-w1kx67w","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251108204924-w1kx67w","updated":"20251108205007"},"Children":[{"ID":"20251108204924-g0die69","Type":"NodeParagraph","Properties":{"id":"20251108204924-g0die69","updated":"20251108205007"},"Children":[{"Type":"NodeText","Data":"激活函数是为了提取非线性关系。神经网络本身就是要抽取特征，让有用信息值变大，让不关注的信息弱化！升维确实能够减少降秩的概率，但不一定"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251019173051-7og2e83","Type":"NodeParagraph","Properties":{"id":"20251019173051-7og2e83","updated":"20251019173051"}},{"ID":"20251108205321-sppu1o8","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251108205321-sppu1o8","updated":"20251108205321"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251108205224-a6nm5rk","Type":"NodeParagraph","Properties":{"id":"20251108205224-a6nm5rk","updated":"20251108205321"},"Children":[{"Type":"NodeText","Data":"FNN的降维会丢信息，Relu激活函数在一定情况下也会丢信息，为什么信息不会越来越少？"}]},{"ID":"20251108205310-ri11f6q","Type":"NodeParagraph","Properties":{"id":"20251108205310-ri11f6q","updated":"20251108205321"},"Children":[{"Type":"NodeText","Data":"别忘了还有一个叫"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"残差的操作，"},{"Type":"NodeText","Data":"会把输入信息直接叠加到输出上！"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251108205224-e0j9bef","Type":"NodeParagraph","Properties":{"id":"20251108205224-e0j9bef","updated":"20251108205224"}},{"ID":"20251019173221-dkoavmm","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251019173221-dkoavmm","updated":"20251019173222"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251019173052-5i3oq2k","Type":"NodeParagraph","Properties":{"id":"20251019173052-5i3oq2k","updated":"20251019173222"},"Children":[{"Type":"NodeText","Data":"为什么transformer的FFN是从512扩到2048，更多更少不行吗？"}]},{"ID":"20251019173118-spv58oe","Type":"NodeParagraph","Properties":{"id":"20251019173118-spv58oe","updated":"20251019173222"},"Children":[{"Type":"NodeText","Data":"理论上升维越多越好，但"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"计算量和显存"},{"Type":"NodeText","Data":"占用也会线性增长。原始Transformer选4倍是实验出来的经验值，后续大量工作验证了这个比例在大多数任务上效果不错。实践中发现低于2倍"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"表达能力"},{"Type":"NodeText","Data":"明显不足，高于8倍"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"边际收益"},{"Type":"NodeText","Data":"很小。这个比例本质上反映了任务复杂度和模型容量之间的匹配关系"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251019172703-ia55pty","Type":"NodeParagraph","Properties":{"id":"20251019172703-ia55pty","updated":"20251019172703"}},{"ID":"20251108205230-bit9ksl","Type":"NodeParagraph","Properties":{"id":"20251108205230-bit9ksl","updated":"20251108205230"}},{"ID":"20251108205227-idzqden","Type":"NodeParagraph","Properties":{"id":"20251108205227-idzqden","updated":"20251108205227"}},{"ID":"20251012183126-4a80w3s","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251012183126-4a80w3s","updated":"20251021154833"},"Children":[{"Type":"NodeText","Data":"Transformer"}]},{"ID":"20251012231421-ikdm4ag","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251012231421-ikdm4ag","updated":"20251012231506"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251012184805-ir1cpk8","Type":"NodeParagraph","Properties":{"id":"20251012184805-ir1cpk8","updated":"20251012231422"},"Children":[{"Type":"NodeText","Data":"Transformer多层的作用？"}]},{"ID":"20251012184817-kk3ap43","Type":"NodeBlockquote","Properties":{"id":"20251012184817-kk3ap43","updated":"20251012231422"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251012184817-zo96whp","Type":"NodeParagraph","Properties":{"id":"20251012184817-zo96whp","updated":"20251012185058"},"Children":[{"Type":"NodeText","Data":" 相似问题： "},{"Type":"NodeTextMark","TextMarkType":"block-ref","TextMarkBlockRefID":"20251012183148-l14zjsp","TextMarkBlockRefSubtype":"d","TextMarkTextContent":"Transformer为何使用多头注意力机制？（为什么不使用一个头）"}]}]},{"ID":"20251012184754-s0mdwx9","Type":"NodeList","ListData":{},"Properties":{"id":"20251012184754-s0mdwx9","updated":"20251012231506"},"Children":[{"ID":"20251012185148-3vjatpq","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185148-3vjatpq","updated":"20251012185148"},"Children":[{"ID":"20251012185148-vcymdhq","Type":"NodeParagraph","Properties":{"id":"20251012185148-vcymdhq","updated":"20251012185214"},"Children":[{"Type":"NodeText","Data":"核心思想：transfomer将基础的"},{"Type":"NodeTextMark","TextMarkType":"code","TextMarkTextContent":"\"块\"(Block/Layer)"},{"Type":"NodeText","Data":"​堆叠多次，形成一个深层网络结构。每一层的输出作为下一层的输入。"}]}]},{"ID":"20251012185218-cowl9jm","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185218-cowl9jm","updated":"20251012231506"},"Children":[{"ID":"20251012185218-3d1ql0n","Type":"NodeParagraph","Properties":{"id":"20251012185218-3d1ql0n","updated":"20251012185221"},"Children":[{"Type":"NodeText","Data":"作用与好处："}]},{"ID":"20251012185221-gcvjzvx","Type":"NodeList","ListData":{},"Properties":{"id":"20251012185221-gcvjzvx","updated":"20251012231506"},"Children":[{"ID":"20251012185221-ts8z3jn","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185221-ts8z3jn","updated":"20251012231436"},"Children":[{"ID":"20251012185221-w1e84ju","Type":"NodeParagraph","Properties":{"id":"20251012185221-w1e84ju","updated":"20251012231428"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"逐步提取和组合更复杂的特征"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"："},{"Type":"NodeText","Data":"模型进行层次化的信息处理"}]},{"ID":"20251012185235-18yz54p","Type":"NodeList","ListData":{},"Properties":{"id":"20251012185235-18yz54p","updated":"20251012231436"},"Children":[{"ID":"20251012185234-oeh3059","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185234-oeh3059","updated":"20251012231431"},"Children":[{"ID":"20251012185234-6skpbw7","Type":"NodeParagraph","Properties":{"id":"20251012185234-6skpbw7","updated":"20251012231431"},"Children":[{"Type":"NodeText","Data":"底层（靠近输入的层）:可能更关注"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"词语本身、局部的上下文"},{"Type":"NodeText","Data":"依赖关系（如短语结构）"}]}]},{"ID":"20251012185237-3lb4df0","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185237-3lb4df0","updated":"20251012231434"},"Children":[{"ID":"20251012185237-0qqf99t","Type":"NodeParagraph","Properties":{"id":"20251012185237-0qqf99t","updated":"20251012231434"},"Children":[{"Type":"NodeText","Data":"中层可以在底层的基础上：开始"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"捕捉更长距离的依赖、句法结构"},{"Type":"NodeText","Data":"或简单的语义关系"}]}]},{"ID":"20251012185255-eyukcyp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185255-eyukcyp","updated":"20251012231436"},"Children":[{"ID":"20251012185255-wrc42ic","Type":"NodeParagraph","Properties":{"id":"20251012185255-wrc42ic","updated":"20251012231436"},"Children":[{"Type":"NodeText","Data":"高层（靠近输出的层）则可以整合来自中下层的信息，进行更"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"复杂的推理"},{"Type":"NodeText","Data":"，理解更高层次的语义、语篇结构、甚至是抽象概念之间的联系"}]}]}]}]},{"ID":"20251012185303-vfp3fjz","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185303-vfp3fjz","updated":"20251012231459"},"Children":[{"ID":"20251012185303-wqbm9rn","Type":"NodeParagraph","Properties":{"id":"20251012185303-wqbm9rn","updated":"20251012231455"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"增加模型容量和表达能力："},{"Type":"NodeText","Data":"网络的深度是"},{"Type":"NodeTextMark","TextMarkType":"strong mark","TextMarkTextContent":"模型复杂度和表达能力"},{"Type":"NodeText","Data":"的关键因素。"}]},{"ID":"20251012185322-ed3gvjm","Type":"NodeList","ListData":{},"Properties":{"id":"20251012185322-ed3gvjm","updated":"20251012231459"},"Children":[{"ID":"20251012185322-lkgzewx","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185322-lkgzewx","updated":"20251012231459"},"Children":[{"ID":"20251012185322-coqpnjm","Type":"NodeParagraph","Properties":{"id":"20251012185322-coqpnjm","updated":"20251012231459"},"Children":[{"Type":"NodeText","Data":"更多的层意味着模型"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"拥有更多的参数和非线性变换"},{"Type":"NodeText","Data":"，能够学习和拟合更复杂的数据模式和函数。"}]}]},{"ID":"20251012185325-x0qp4sf","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185325-x0qp4sf","updated":"20251012185325"},"Children":[{"ID":"20251012185325-ruyrlgy","Type":"NodeParagraph","Properties":{"id":"20251012185325-ruyrlgy","updated":"20251012185327"},"Children":[{"Type":"NodeText","Data":"对于需要理解复杂语言现象的任务(如机器翻译、文本生成、问答)，深层结构是必不可少的"}]}]}]}]},{"ID":"20251012185328-ub9pltc","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251012185328-ub9pltc","updated":"20251012231506"},"Children":[{"ID":"20251012185328-0w38rzi","Type":"NodeParagraph","Properties":{"id":"20251012185328-0w38rzi","updated":"20251012231506"},"Children":[{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"逐层精炼："},{"Type":"NodeText","Data":"每一层都可以看作是"},{"Type":"NodeTextMark","TextMarkType":"u mark","TextMarkTextContent":"对上一层输出表示的一次\"精炼”"},{"Type":"NodeText","Data":"(refinement)。通过多层堆叠，信息在网络中逐层传播和处理，使得最"},{"Type":"NodeTextMark","TextMarkType":"u","TextMarkTextContent":"终的表示能够充分融入上下文信息"},{"Type":"NodeText","Data":"，并达到"},{"Type":"NodeTextMark","TextMarkType":"u","TextMarkTextContent":"较高的抽象"},{"Type":"NodeText","Data":"水平。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251012184754-b56aqxw","Type":"NodeParagraph","Properties":{"id":"20251012184754-b56aqxw","updated":"20251012184754"}},{"ID":"20251019174613-t17fc8x","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251019174613-t17fc8x","updated":"20251019174613"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251019172646-epcj8qn","Type":"NodeParagraph","Properties":{"id":"20251019172646-epcj8qn","updated":"20251019174613"},"Children":[{"Type":"NodeText","Data":"为何在Softmax前进行Masking？"}]},{"ID":"20251019173936-ueum1y7","Type":"NodeParagraph","Properties":{"id":"20251019173936-ueum1y7","updated":"20251019174613"},"Children":[{"Type":"NodeText","Data":"Masking（掩码）操作在Softmax之前进行，其核心目的是为了防止模型在训练时“作弊”，即"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"防止其接触到不应被看到的信息"},{"Type":"NodeText","Data":"。"}]},{"ID":"20251019174010-r3q1ad3","Type":"NodeParagraph","Properties":{"id":"20251019174010-r3q1ad3","updated":"20251019174613"},"Children":[{"Type":"NodeText","Data":"具体操作：将某些位置的得分替换为一个"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"极小的负数（如 -1e9）"},{"Type":"NodeText","Data":"，使得这些位置在后续的Softmax计算中"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"概率趋近于零"},{"Type":"NodeText","Data":"，从而被有效忽略。"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251019172646-m1ipi6y","Type":"NodeParagraph","Properties":{"id":"20251019172646-m1ipi6y","updated":"20251019172646"}},{"ID":"20251019175003-eljowge","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251019175003-eljowge","updated":"20251019175004"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251019174616-38zy2i9","Type":"NodeParagraph","Properties":{"id":"20251019174616-38zy2i9","updated":"20251019175004"},"Children":[{"Type":"NodeText","Data":"讲一讲Transofmer中有哪些类型的mask？"}]},{"ID":"20251019172646-n55x4my","Type":"NodeList","ListData":{},"Properties":{"id":"20251019172646-n55x4my","updated":"20251019175004"},"Children":[{"ID":"20251019174640-m9wrtp3","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174640-m9wrtp3","updated":"20251019174640"},"Children":[{"ID":"20251019174640-4irnvrz","Type":"NodeParagraph","Properties":{"id":"20251019174640-4irnvrz","updated":"20251019174640"},"Children":[{"Type":"NodeText","Data":"编码器中的填充掩码（Padding Mask）"}]},{"ID":"20251019174649-dgr0sxe","Type":"NodeList","ListData":{},"Properties":{"id":"20251019174649-dgr0sxe","updated":"20251019174649"},"Children":[{"ID":"20251019174649-l4vbh72","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174649-l4vbh72","updated":"20251019174649"},"Children":[{"ID":"20251019174649-peznz4b","Type":"NodeParagraph","Properties":{"id":"20251019174649-peznz4b","updated":"20251019174649"},"Children":[{"Type":"NodeText","Data":"短序列通常会被填充（Padding）至同一长度"}]}]},{"ID":"20251019174703-687d4ys","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174703-687d4ys","updated":"20251019174703"},"Children":[{"ID":"20251019174703-w4ixb9q","Type":"NodeParagraph","Properties":{"id":"20251019174703-w4ixb9q","updated":"20251019174703"},"Children":[{"Type":"NodeText","Data":"填充掩码的作用就是在计算注意力时，屏蔽掉这些填充位置。"}]}]}]}]},{"ID":"20251019174704-y132dnv","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174704-y132dnv","updated":"20251019174704"},"Children":[{"ID":"20251019174704-de7k1z9","Type":"NodeParagraph","Properties":{"id":"20251019174704-de7k1z9","updated":"20251019174724"},"Children":[{"Type":"NodeText","Data":"解码器中的自回归掩码（Auto-regressive Mask / Look-ahead Mask）："}]},{"ID":"20251019174725-4k27z7l","Type":"NodeList","ListData":{},"Properties":{"id":"20251019174725-4k27z7l","updated":"20251019174725"},"Children":[{"ID":"20251019174725-sz0awen","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174725-sz0awen","updated":"20251019174725"},"Children":[{"ID":"20251019174725-gu42dfw","Type":"NodeParagraph","Properties":{"id":"20251019174725-gu42dfw","updated":"20251019174737"},"Children":[{"Type":"NodeText","Data":"在训练或生成时，解码器应该仅基于已经生成的输出来预测下一个令牌，而不能“偷看”未来的答案。"}]}]},{"ID":"20251019174740-24e8g8p","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174740-24e8g8p","updated":"20251019174740"},"Children":[{"ID":"20251019174740-ia03rl5","Type":"NodeParagraph","Properties":{"id":"20251019174740-ia03rl5","updated":"20251019174740"},"Children":[{"Type":"NodeText","Data":"自回归掩码是一个下三角矩阵（主对角线也为0或从1开始），它确保了在计算第 i 个位置的注意力时，只能关注到第 1 到第 i 个位置（在之前的位置），而无法关注到 i+1 及之后的位置。这强制模型遵守了序列生成的因果律。"}]}]}]}]},{"ID":"20251019174743-66o6ymh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174743-66o6ymh","updated":"20251019174936"},"Children":[{"ID":"20251019174743-m6135rj","Type":"NodeParagraph","Properties":{"id":"20251019174743-m6135rj","updated":"20251019174758"},"Children":[{"Type":"NodeText","Data":"编码-解码器掩码（Encoder-Decoder Mask）："}]},{"ID":"20251019174759-556p4qb","Type":"NodeList","ListData":{},"Properties":{"id":"20251019174759-556p4qb","updated":"20251019174936"},"Children":[{"ID":"20251019174759-vp3ynla","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251019174759-vp3ynla","updated":"20251019174936"},"Children":[{"ID":"20251019174759-cwh4rx0","Type":"NodeParagraph","Properties":{"id":"20251019174759-cwh4rx0","updated":"20251019174936"},"Children":[{"Type":"NodeText","Data":"编码器的输入可能包含填充符，其输出的Memory也对应着这些无效位置。解码器在通过交叉注意力（Cross-Attention）机制访问Memory时，必须屏蔽掉这些位置，避免受到输入序列中填充噪声的影响。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251019174632-qdvpm6m","Type":"NodeParagraph","Properties":{"id":"20251019174632-qdvpm6m","updated":"20251019174632"}},{"ID":"20251021154830-joq51ux","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20251021154830-joq51ux","updated":"20251021162520"},"Children":[{"Type":"NodeText","Data":"层归一化"}]},{"ID":"20251019235531-wwczqha","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251019235531-wwczqha","updated":"20251021153530"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251019174632-gr42rf6","Type":"NodeParagraph","Properties":{"id":"20251019174632-gr42rf6","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"为什么大模型常常选用Pre-norm而不是Post-Norm？"}]},{"ID":"20251019235359-6lbb72i","Type":"NodeParagraph","Properties":{"id":"20251019235359-6lbb72i","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"首先明确两者核心区别：Pre-Norm是先对输入做归一化，再进注意力/FFN子模块，最后接残差；Post-Norm是先算子模块、接残差，最后对结果归一化。现在大模型更倾向选Pre-Norm，核心原因有三点："}]},{"ID":"20251019235359-u43cc3b","Type":"NodeParagraph","Properties":{"id":"20251019235359-u43cc3b","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"第一是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"训练更稳定"},{"Type":"NodeText","Data":"。Post-Norm的"},{"Type":"NodeTextMark","TextMarkType":"strong u","TextMarkTextContent":"归一化作用在残差路径上"},{"Type":"NodeText","Data":"，深层模型里"},{"Type":"NodeTextMark","TextMarkType":"mark strong","TextMarkTextContent":"梯度会因多层缩放累积而消失"},{"Type":"NodeText","Data":"，18层以上就很难训；Pre-Norm的残差路径没被缩放，"},{"Type":"NodeTextMark","TextMarkType":"strong u","TextMarkTextContent":"梯度能直接传到底层"},{"Type":"NodeText","Data":"，支持96层（如GPT-3）甚至上百层模型稳定训练。"}]},{"ID":"20251021152822-iwwvfmk","Type":"NodeBlockquote","Properties":{"id":"20251021152822-iwwvfmk","updated":"20251021153530"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251021152825-j7i11zs","Type":"NodeParagraph","Properties":{"id":"20251021152825-j7i11zs","updated":"20251021153039"},"Children":[{"Type":"NodeText","Data":" Norm的计算公式中，梯度会引入缩放因子（依赖输入的方差和均值），如果x的分布变化，那么梯度也会变化很大，导致梯度变化不稳定（方差大，梯度缩小；方差小梯度增大）"}]},{"ID":"20251021153039-nqh00j3","Type":"NodeParagraph","Properties":{"id":"20251021153039-nqh00j3","updated":"20251021153530"},"Children":[{"Type":"NodeText","Data":"在post-norm中，所有项（包括残差连接）都会被缩放调整，低层的梯度可能会被指数级衰减；而pre-norm里面，缩放因子仅影响内层模块，不影响外层残差连接的路径，保持了原始数据梯度的稳定性"}]}]},{"ID":"20251019235359-o1n0pg3","Type":"NodeParagraph","Properties":{"id":"20251019235359-o1n0pg3","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"第二是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"工程效率高"},{"Type":"NodeText","Data":"。Post-Norm需要精细调学习率"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"预热、初始化，成本高"},{"Type":"NodeText","Data":"；Pre-Norm不用这些，直接"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"用大学习率就能训"},{"Type":"NodeText","Data":"，还能和RMSNorm（如Llama 2）搭配减少计算量，训练速度快40%左右，适合大模型的工程落地。"}]},{"ID":"20251019235359-u22pnfe","Type":"NodeParagraph","Properties":{"id":"20251019235359-u22pnfe","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"第三是"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"支持模型扩展"},{"Type":"NodeText","Data":"。大模型靠堆层数提性能，Post-Norm层数受限；Pre-Norm能轻松做深层，哪怕千亿参数模型（如GLM-130B）也能训，而且"},{"Type":"NodeTextMark","TextMarkType":"u","TextMarkTextContent":"增加层数带来的性能提升，远超过它可能的表征损失"},{"Type":"NodeText","Data":"。"}]},{"ID":"20251019235359-41hn0nr","Type":"NodeParagraph","Properties":{"id":"20251019235359-41hn0nr","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"另外，Post-Norm只在早期小模型（如BERT）里用，现在主流的GPT-3、Llama 2、Qwen等都用Pre-Norm，实践也证明它在大模型场景下性价比更高。"}]},{"ID":"20251019235359-dc8adzl","Type":"NodeParagraph","Properties":{"id":"20251019235359-dc8adzl","updated":"20251019235532"},"Children":[{"Type":"NodeText","Data":"总结来说，Pre-Norm是大模型在“能训得深、训得快、能落地”上的最优选择，所以成了现在的主流。"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251019174632-gup3yz1","Type":"NodeParagraph","Properties":{"id":"20251019174632-gup3yz1","updated":"20251019174632"}},{"ID":"20251021154810-695a7ut","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251021154810-695a7ut","updated":"20251021160851"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251019235815-v0fk4w6","Type":"NodeParagraph","Properties":{"id":"20251019235815-v0fk4w6","updated":"20251021154811"},"Children":[{"Type":"NodeText","Data":"请简述 LayerNorm 的正向计算逻辑，并解释其反向传播过程中 “缩放因子” 产生的原因。"}]},{"ID":"20251021153707-sfawc4i","Type":"NodeParagraph","Properties":{"id":"20251021153707-sfawc4i","updated":"20251021160851"},"Children":[{"Type":"NodeText","Data":"LayerNorm的核心是通过标准化调整输入分布，将 x 的均值和方差固定为预设值（通常为 0 和 1）。同时引入可学习参数保留模型表征能力，具体如下："}]},{"ID":"20251021153812-5bhc36b","Type":"NodeParagraph","Properties":{"id":"20251021153812-5bhc36b","updated":"20251021154811"},"Children":[{"Type":"NodeText","Data":"正向计算逻辑：首先计算输入特征"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"x"},{"Type":"NodeText","Data":"的均值"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"μ=mean(x)"},{"Type":"NodeText","Data":"和标准差"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\sigma=std(x)"},{"Type":"NodeText","Data":",然后对"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"x"},{"Type":"NodeText","Data":"进行标准化"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"(x-\\mu)/\\sigma"},{"Type":"NodeText","Data":"，最后通过可学习参数"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\gamma"},{"Type":"NodeText","Data":"(缩放)和"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\beta"},{"Type":"NodeBackslash","Data":"span","Children":[{"Type":"NodeText","Data":"("}]},{"Type":"NodeText","Data":"偏移)调整分布，公式为： "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"Norm(x)=\\frac{x-\\mu }{\\sigma }·\\gamma +\\beta"},{"Type":"NodeText","Data":"。"}]},{"ID":"20251021154031-yhppeui","Type":"NodeParagraph","Properties":{"id":"20251021154031-yhppeui","updated":"20251021154811"},"Children":[{"Type":"NodeText","Data":"缩放因子产生的原因：反向传播时，需通过链式法则计算 "},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\frac{\\partial\\text{Norm}(x)}{\\partial x}"},{"Type":"NodeText","Data":"。由于"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\mu"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\sigma"},{"Type":"NodeText","Data":"均依赖输入x，求导过程中会引入项"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\frac{1}{\\sigma}"},{"Type":"NodeText","Data":"是，其随输入x分布动态变化，导致"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\frac{1}{\\sigma}"},{"Type":"NodeText","Data":"​也随之波动，相当于对梯度进行了“动态比例调整”，因此将，及相关项统称为“缩放因子”。"}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251019235837-4c2sfy3","Type":"NodeParagraph","Properties":{"id":"20251019235837-4c2sfy3","updated":"20251021153738"},"Children":[{"Type":"NodeTextMark","TextMarkType":"a","TextMarkAHref":"https://juejin.cn/post/7542308569639731236","TextMarkATitle":"大模型面试题剖析:Pre-Norm与Post-Norm的对比及当代大模型选择Pre-Norm的原因","TextMarkTextContent":"大模型面试题剖析:Pre-Norm与Post-Norm的对比及当代大模型选择Pre-Norm的原因"}]},{"ID":"20251019235815-x1os2fl","Type":"NodeParagraph","Properties":{"id":"20251019235815-x1os2fl","updated":"20251019235815"}},{"ID":"20251021154815-5e5kmz0","Type":"NodeParagraph","Properties":{"id":"20251021154815-5e5kmz0","updated":"20251021154815"}},{"ID":"20251021155500-nsao9rd","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251021155500-nsao9rd","updated":"20251021155501"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251021154826-nwyfoe4","Type":"NodeParagraph","Properties":{"id":"20251021154826-nwyfoe4","updated":"20251021155501"},"Children":[{"Type":"NodeText","Data":"为何 Post-Norm 在训练深层 Transformer 模型时易出现稳定性问题？Pre-Norm 通过何种机制解决这一问题？"}]},{"ID":"20251021154826-5p007h2","Type":"NodeParagraph","Properties":{"id":"20251021154826-5p007h2","updated":"20251021155501"},"Children":[{"Type":"NodeText","Data":"Post-Norm 的稳定性问题源于梯度传递中的 “缩放因子累积”，而 Pre-Norm 通过 “"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"梯度路径分离"},{"Type":"NodeText","Data":"” 机制解决该问题，具体分析如下："}]},{"ID":"20251021154928-gjf0d6c","Type":"NodeList","ListData":{},"Properties":{"id":"20251021154928-gjf0d6c","updated":"20251021155501"},"Children":[{"ID":"20251021154929-mlpi4v2","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021154929-mlpi4v2","updated":"20251021154958"},"Children":[{"ID":"20251021154929-1epxr7g","Type":"NodeParagraph","Properties":{"id":"20251021154929-1epxr7g","updated":"20251021154929"},"Children":[{"Type":"NodeText","Data":"Post-Norm 的稳定性问题：深层模型中，梯度需经过多层子模块与归一化操作传递。"}]},{"ID":"20251021154933-7z4nqnh","Type":"NodeList","ListData":{},"Properties":{"id":"20251021154933-7z4nqnh","updated":"20251021154958"},"Children":[{"ID":"20251021154933-3ueumnp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021154933-3ueumnp","updated":"20251021154958"},"Children":[{"ID":"20251021154933-02yhima","Type":"NodeParagraph","Properties":{"id":"20251021154933-02yhima","updated":"20251021154958"},"Children":[{"Type":"NodeText","Data":"Post-Norm 的梯度公式中，归一化产生的缩放因子（含"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\frac{1}{\\sigma}"},{"Type":"NodeText","Data":"项）会作用于整个梯度路径 —— 随着层数增加（如超过 20 层），"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"缩放因子的累积效应会导致低层梯度呈指数级衰减"},{"Type":"NodeText","Data":"，最终低层"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"参数更新幅度极小"},{"Type":"NodeText","Data":"，模型训练"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"收敛困难"},{"Type":"NodeText","Data":"甚至失败。"}]}]}]}]}]},{"ID":"20251021154826-tlz8ftx","Type":"NodeList","ListData":{},"Properties":{"id":"20251021154826-tlz8ftx","updated":"20251021155501"},"Children":[{"ID":"20251021155013-tzj8iv5","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021155013-tzj8iv5","updated":"20251021155210"},"Children":[{"ID":"20251021155013-vvwrf69","Type":"NodeParagraph","Properties":{"id":"20251021155013-vvwrf69","updated":"20251021155013"},"Children":[{"Type":"NodeText","Data":"Pre-Norm 的解决机制：Pre-Norm 的梯度传递分为两条路径："}]},{"ID":"20251021155029-dol8giz","Type":"NodeList","ListData":{},"Properties":{"id":"20251021155029-dol8giz","updated":"20251021155210"},"Children":[{"ID":"20251021155129-ewqny1w","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021155129-ewqny1w","updated":"20251021155129"},"Children":[{"ID":"20251021155129-o7auls5","Type":"NodeBlockquote","Properties":{"id":"20251021155129-o7auls5","updated":"20251021155130"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e"},{"ID":"20251021155130-i2embr7","Type":"NodeParagraph","Properties":{"id":"20251021155130-i2embr7","updated":"20251021155201"},"Children":[{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"x=x+subModual(LN(x))"},{"Type":"NodeText","Data":" 这是pre-norm的计算，第一个x是直接路径，第二部分是子模块"}]}]}]},{"ID":"20251021155016-a7vval8","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021155016-a7vval8","updated":"20251021155016"},"Children":[{"ID":"20251021155016-sprcjy0","Type":"NodeParagraph","Properties":{"id":"20251021155016-sprcjy0","updated":"20251021155023"},"Children":[{"Type":"NodeText","Data":" 直接路径：残差连接直接传递原始输入的梯度（即"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\left. \\frac{\\partial L}{\\partial x} \\right|_{\\text{直接路径}}"},{"Type":"NodeText","Data":"），该路径完全不经过归一化操作，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"无缩放因子干扰"},{"Type":"NodeText","Data":"，可稳定传递至低层；"}]}]},{"ID":"20251021155025-8k9as3j","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021155025-8k9as3j","updated":"20251021155025"},"Children":[{"ID":"20251021155025-0nmd1u4","Type":"NodeParagraph","Properties":{"id":"20251021155025-0nmd1u4","updated":"20251021155025"},"Children":[{"Type":"NodeText","Data":"子模块路径：经过归一化与子模块的梯度（含缩放因子）仅作用于子模块输入，不影响核心的直接路径梯度。"}]}]},{"ID":"20251021155206-3zzj7br","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021155206-3zzj7br","updated":"20251021155210"},"Children":[{"ID":"20251021155206-naapamh","Type":"NodeParagraph","Properties":{"id":"20251021155206-naapamh","updated":"20251021155210"},"Children":[{"Type":"NodeText","Data":" 两条路径分离确保了"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"低层梯度的有效传递"},{"Type":"NodeText","Data":"，提升了深层模型的训练稳定性。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251021154826-8orw6zu","Type":"NodeParagraph","Properties":{"id":"20251021154826-8orw6zu","updated":"20251021155213"}},{"ID":"20251021160302-dk2ppi0","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251021160302-dk2ppi0","updated":"20251021160302"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251021160203-70l094h","Type":"NodeParagraph","Properties":{"id":"20251021160203-70l094h","updated":"20251021160302"},"Children":[{"Type":"NodeText","Data":"使用 Post-Norm 训练深层模型时，层数增加会引发哪些具体问题？可通过哪些优化技巧缓解？"}]},{"ID":"20251021160203-30otd4t","Type":"NodeParagraph","Properties":{"id":"20251021160203-30otd4t","updated":"20251021160302"},"Children":[{"Type":"NodeText","Data":"Post-Norm 随层数增加的核心问题的是 “"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"梯度衰减"},{"Type":"NodeText","Data":"” 与 “"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"训练复杂度上升"},{"Type":"NodeText","Data":"”，具体及缓解技巧如下："}]},{"ID":"20251021160215-y3l7uew","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160215-y3l7uew","updated":"20251021160302"},"Children":[{"ID":"20251021160215-3tvvbcx","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160215-3tvvbcx","updated":"20251021160245"},"Children":[{"ID":"20251021160215-1m9p1it","Type":"NodeParagraph","Properties":{"id":"20251021160215-1m9p1it","updated":"20251021160215"},"Children":[{"Type":"NodeText","Data":" 层数增加引发的问题："}]},{"ID":"20251021160218-3mrrsts","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160218-3mrrsts","updated":"20251021160245"},"Children":[{"ID":"20251021160217-ptrddvh","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160217-ptrddvh","updated":"20251021160217"},"Children":[{"ID":"20251021160217-8po0fuv","Type":"NodeParagraph","Properties":{"id":"20251021160217-8po0fuv","updated":"20251021160223"},"Children":[{"Type":"NodeText","Data":"梯度衰减："},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"低层梯度经多层缩放因子累积后大幅减小"},{"Type":"NodeText","Data":"，参数更新失效，模型难以收敛；"}]}]},{"ID":"20251021160226-q4x3pf9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160226-q4x3pf9","updated":"20251021160226"},"Children":[{"ID":"20251021160226-1dz5poa","Type":"NodeParagraph","Properties":{"id":"20251021160226-1dz5poa","updated":"20251021160226"},"Children":[{"Type":"NodeText","Data":" 训练门槛高：需依赖复杂调参策略才能维持基本稳定性，否则易出现训练震荡或发散。"}]}]},{"ID":"20251021160230-38dvo8n","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160230-38dvo8n","updated":"20251021160245"},"Children":[{"ID":"20251021160230-zvi9rsw","Type":"NodeParagraph","Properties":{"id":"20251021160230-zvi9rsw","updated":"20251021160238"},"Children":[{"Type":"NodeText","Data":" 缓解技巧："}]},{"ID":"20251021160245-tcs8tcf","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160245-tcs8tcf","updated":"20251021160245"},"Children":[{"ID":"20251021160245-m62sbco","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160245-m62sbco","updated":"20251021160245"},"Children":[{"ID":"20251021160245-38y6ctn","Type":"NodeParagraph","Properties":{"id":"20251021160245-38y6ctn","updated":"20251021160245"},"Children":[{"Type":"NodeText","Data":" 学习率预热（Warmup）：训练初期采用"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"较小学习率"},{"Type":"NodeText","Data":"，逐步提升至目标值，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"避免初始阶段梯度波动过大"},{"Type":"NodeText","Data":"；"}]}]},{"ID":"20251021160239-5qp669m","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160239-5qp669m","updated":"20251021160239"},"Children":[{"ID":"20251021160239-mexwbog","Type":"NodeParagraph","Properties":{"id":"20251021160239-mexwbog","updated":"20251021160239"},"Children":[{"Type":"NodeText","Data":"精细参数初始化：采用 Xavier 或 He 初始化等策略，确保各层输入输出分布稳定，减少"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\sigma"},{"Type":"NodeText","Data":"的剧烈波动；"}]}]},{"ID":"20251021160242-lkxqbxk","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160242-lkxqbxk","updated":"20251021160242"},"Children":[{"ID":"20251021160242-x1sfndv","Type":"NodeParagraph","Properties":{"id":"20251021160242-x1sfndv","updated":"20251021160242"},"Children":[{"Type":"NodeText","Data":"增强正则化：引入 Dropout、Weight Decay 等正则化手段，抑制参数过拟合与梯度异常。"}]}]},{"ID":"20251021160253-jjsex70","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160253-jjsex70","updated":"20251021160253"},"Children":[{"ID":"20251021160253-uoj0sku","Type":"NodeParagraph","Properties":{"id":"20251021160253-uoj0sku","updated":"20251021160253"},"Children":[{"Type":"NodeText","Data":"但需注意：即使采用上述技巧，Post-Norm 仍难以支持 30 层以上的深层模型，灵活性远低于 Pre-Norm。"}]}]}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251021160203-t2nslo0","Type":"NodeParagraph","Properties":{"id":"20251021160203-t2nslo0","updated":"20251021160257"}},{"ID":"20251021160641-ashx7qg","Type":"NodeSuperBlock","Properties":{"custom-riff-decks":"20230218211946-2kw8jgx","id":"20251021160641-ashx7qg","updated":"20251021162520"},"Children":[{"Type":"NodeSuperBlockOpenMarker"},{"Type":"NodeSuperBlockLayoutMarker","Data":"row"},{"ID":"20251021160542-5ur5cju","Type":"NodeParagraph","Properties":{"id":"20251021160542-5ur5cju","updated":"20251021160641"},"Children":[{"Type":"NodeText","Data":"Pre-Norm 存在 “表征坍塌” 风险（即特征多样性下降），实际工程中可通过哪些方案缓解？"}]},{"ID":"20251021160542-r70w20q","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160542-r70w20q","updated":"20251021162520"},"Children":[{"ID":"20251021160918-p3s76na","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160918-p3s76na","updated":"20251021160951"},"Children":[{"ID":"20251021160918-hnqr0zi","Type":"NodeParagraph","Properties":{"id":"20251021160918-hnqr0zi","updated":"20251021160927"},"Children":[{"Type":"NodeText","Data":"什么是表征坍塌"}]},{"ID":"20251021160930-v23v7c4","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160930-v23v7c4","updated":"20251021160951"},"Children":[{"ID":"20251021160931-ox2jx8g","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160931-ox2jx8g","updated":"20251021160951"},"Children":[{"ID":"20251021160931-2txp18y","Type":"NodeParagraph","Properties":{"id":"20251021160931-2txp18y","updated":"20251021160946"},"Children":[{"Type":"NodeText","Data":"归一化操作将 x 的均值和方差固定为预设值（通常为 0 和 1），导致"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"不同样本"},{"Type":"NodeText","Data":"的特征在进入"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"子模块"},{"Type":"NodeText","Data":"（如注意力或 FFN）前被"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"强行拉平"},{"Type":"NodeText","Data":"。这种约束虽能"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"稳定训练"},{"Type":"NodeText","Data":"，但也可能"},{"Type":"NodeTextMark","TextMarkType":"strong","TextMarkTextContent":"过度压缩特征空间的多样性"},{"Type":"NodeText","Data":"。"}]},{"ID":"20251021160948-eu5ocjb","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160948-eu5ocjb","updated":"20251021160951"},"Children":[{"ID":"20251021160947-5yvwkaa","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160947-5yvwkaa","updated":"20251021160951"},"Children":[{"ID":"20251021160947-x5qhywo","Type":"NodeParagraph","Properties":{"id":"20251021160947-x5qhywo","updated":"20251021160951"},"Children":[{"Type":"NodeText","Data":"例如，当多个样本的输入经过归一化后，其特征分布高度趋同，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"子模块难以学习到差异化的模式"},{"Type":"NodeText","Data":"，最终导致深层特征同质化。"}]}]}]}]},{"ID":"20251021160545-g6cp89o","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160545-g6cp89o","updated":"20251021160545"},"Children":[{"ID":"20251021160545-jlf9xun","Type":"NodeParagraph","Properties":{"id":"20251021160545-jlf9xun","updated":"20251021160551"},"Children":[{"Type":"NodeText","Data":"Pre-Norm 的表征坍塌源于 “"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"归一化前置导致的输入约束过强"},{"Type":"NodeText","Data":"”，"}]}]}]}]},{"ID":"20251021160554-t4oxxjp","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160554-t4oxxjp","updated":"20251021162520"},"Children":[{"ID":"20251021160554-ms0e4h6","Type":"NodeParagraph","Properties":{"id":"20251021160554-ms0e4h6","updated":"20251021160554"},"Children":[{"Type":"NodeText","Data":"工程中常用以下 4 类缓解方案："}]},{"ID":"20251021160557-1ma3wk3","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160557-1ma3wk3","updated":"20251021162520"},"Children":[{"ID":"20251021160556-vldibo2","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160556-vldibo2","updated":"20251021160556"},"Children":[{"ID":"20251021160556-hehoh67","Type":"NodeParagraph","Properties":{"id":"20251021160556-hehoh67","updated":"20251021160556"},"Children":[{"Type":"NodeText","Data":"双残差连接设计：在子模块内部（如 Attention 或 FFN）增加额外残差路径，"}]},{"ID":"20251021160600-7dt2qaj","Type":"NodeList","ListData":{},"Properties":{"id":"20251021160600-7dt2qaj","updated":"20251021160600"},"Children":[{"ID":"20251021160559-y2xozk9","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160559-y2xozk9","updated":"20251021160559"},"Children":[{"ID":"20251021160559-7o77st0","Type":"NodeParagraph","Properties":{"id":"20251021160559-7o77st0","updated":"20251021160603"},"Children":[{"Type":"NodeText","Data":"例如在 Attention 子模块中添加 “"},{"Type":"NodeTextMark","TextMarkType":"em","TextMarkTextContent":"Norm (x)→Attention (Norm (x))→Norm (x)+Attention (Norm (x))"},{"Type":"NodeText","Data":"” 的内层残差，增强特征多样性；"}]}]}]}]},{"ID":"20251021160608-m4zt8oy","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160608-m4zt8oy","updated":"20251021162520"},"Children":[{"ID":"20251021160608-kz8wobz","Type":"NodeParagraph","Properties":{"id":"20251021160608-kz8wobz","updated":"20251021162520"},"Children":[{"Type":"NodeText","Data":"LayerNorm 参数约束：初始化时将 LayerNorm 的缩放参数"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\gamma"},{"Type":"NodeText","Data":"设为 1，通过 Weight Decay 正则化限制"},{"Type":"NodeTextMark","TextMarkType":"inline-math","TextMarkInlineMathContent":"\\gamma"},{"Type":"NodeText","Data":"的更新范围，"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"避免"},{"Type":"NodeTextMark","TextMarkType":"inline-math mark","TextMarkInlineMathContent":"\\gamma"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"过小导致特征方差丢失"},{"Type":"NodeText","Data":"；随机破坏特征分布的单一性，迫使模型学习更鲁棒的表示。"}]}]},{"ID":"20251021160621-xsu712q","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160621-xsu712q","updated":"20251021160621"},"Children":[{"ID":"20251021160621-kmge6w1","Type":"NodeParagraph","Properties":{"id":"20251021160621-kmge6w1","updated":"20251021160628"},"Children":[{"Type":"NodeText","Data":"替换归一化方式：采用约束更宽松的归一化技术，如 RMSNorm（仅计算均方根而非完整方差），"},{"Type":"NodeTextMark","TextMarkType":"mark","TextMarkTextContent":"减少对输入特征的过度压制"},{"Type":"NodeText","Data":"（如 Llama 系列采用 Pre-Norm+RMSNorm 组合）；"}]}]},{"ID":"20251021160630-qro78p7","Type":"NodeListItem","ListData":{"BulletChar":42,"Marker":"Kg=="},"Properties":{"id":"20251021160630-qro78p7","updated":"20251021160630"},"Children":[{"ID":"20251021160630-vuybz7h","Type":"NodeParagraph","Properties":{"id":"20251021160630-vuybz7h","updated":"20251021160630"},"Children":[{"Type":"NodeText","Data":"增强正则化：引入 Dropout（随机失活部分特征）或 LayerDrop（随机失活部分子模块），打破特征分布的单一性，提升表征多样性。"}]}]}]}]}]},{"Type":"NodeSuperBlockCloseMarker"}]},{"ID":"20251021160542-h2kd4ow","Type":"NodeParagraph","Properties":{"id":"20251021160542-h2kd4ow","updated":"20251021160635"}},{"ID":"20251019235815-974mtwo","Type":"NodeParagraph","Properties":{"id":"20251019235815-974mtwo","updated":"20251019235815"}},{"ID":"20251012162711-qdpdez2","Type":"NodeParagraph","Properties":{"id":"20251012162711-qdpdez2","updated":"20251012162712"},"Children":[{"Type":"NodeText","Data":"什么因素会导致模型训练不稳定，什么激活函数会导致训练不稳定，什么原因会导致梯度消失/爆炸"}]},{"ID":"20251012163359-youj204","Type":"NodeParagraph","Properties":{"id":"20251012163359-youj204","updated":"20251012163359"}},{"ID":"20251012163400-xldb6l5","Type":"NodeParagraph","Properties":{"id":"20251012163400-xldb6l5","updated":"20251012163400"}},{"ID":"20251012163526-pw5ab6n","Type":"NodeParagraph","Properties":{"id":"20251012163526-pw5ab6n","updated":"20251012163529"},"Children":[{"Type":"NodeText","Data":"1.Transformer为何使用多头注意力机制？（为什么不使用一个头）\n2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？ （注意和第一个问题的区别）\n3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n5.在计算attention score的时候如何对padding做mask操作？\n6.为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n7.大概讲一下Transformer的Encoder模块？\n8.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n9.简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n10.你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n11.简单讲一下Transformer中的残差结构以及意义。\n12.为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n13.简答讲一下BatchNorm技术，以及它的优缺点。\n14.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\n15.Encoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\n16.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\n17.Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n19.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n20解码端的残差结构有没有把后续未被看见的mask信息添加进来，造成信息的泄露。"}]},{"ID":"20251012163400-3f1dued","Type":"NodeParagraph","Properties":{"id":"20251012163400-3f1dued","updated":"20251015232251"},"Children":[{"Type":"NodeText","Data":"llm计算loss为什么用交叉熵而不是mse？"}]},{"ID":"20251012163400-5894i0x","Type":"NodeParagraph","Properties":{"id":"20251012163400-5894i0x","updated":"20251012163400"}}]}